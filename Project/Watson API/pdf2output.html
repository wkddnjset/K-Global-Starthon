<html><head>
<meta http-equiv="Content-Type" content="text/html; charset=euc-kr">
</head><body>
<span style="position:absolute; border: gray 1px solid; left:0px; top:50px; width:612px; height:792px;"></span>
<div style="position:absolute; top:50px;"><a name="1">Page 1</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:252px; top:330px; width:107px; height:17px;"><span style="font-family: HFZHZX+CMR17; font-size:17px">Master¡¯s Thesis
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:175px; top:352px; width:260px; height:17px;"><span style="font-family: HFZHZX+CMR17; font-size:17px">Deep Learning for Visual Recognition
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:178px; top:424px; width:255px; height:11px;"><span style="font-family: PYSDXF+CMR12; font-size:11px">Supervised by Nicolas Thome and Matthieu Cord
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:271px; top:410px; width:68px; height:11px;"><span style="font-family: PYSDXF+CMR12; font-size:11px">Remi Cadene
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:224px; top:461px; width:163px; height:13px;"><span style="font-family: PYSDXF+CMR12; font-size:11px">Wednesday 7</span><span style="font-family: KEWQQA+CMR8; font-size:7px">th </span><span style="font-family: PYSDXF+CMR12; font-size:11px">September, 2016
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:262px; width:22px; height:40px;"><span style="font-family: Times-Roman; font-size:10px">6
<br>1
<br>0
<br>2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:302px; width:22px; height:58px;"><span style="font-family: Times-Roman; font-size:5px"> 
<br></span><span style="font-family: Times-Roman; font-size:5px">t
<br></span><span style="font-family: Times-Roman; font-size:8px">c
<br></span><span style="font-family: Times-Roman; font-size:14px">O
<br></span><span style="font-family: Times-Roman; font-size:10px">8
<br>1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:336px; width:22px; height:5px;"><span style="font-family: Times-Roman; font-size:5px"> 
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:361px; width:22px; height:16px;"><span style="font-family: Times-Roman; font-size:5px"> 
<br> 
<br></span><span style="font-family: Times-Roman; font-size:6px">]
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:377px; width:22px; height:66px;"><span style="font-family: Times-Roman; font-size:14px">V
<br></span><span style="font-family: Times-Roman; font-size:13px">C
<br></span><span style="font-family: Times-Roman; font-size:5px">.
<br></span><span style="font-family: Times-Roman; font-size:7px">s
<br></span><span style="font-family: Times-Roman; font-size:8px">c
<br></span><span style="font-family: Times-Roman; font-size:6px">[
<br></span><span style="font-family: Times-Roman; font-size:5px"> 
<br> 
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:443px; width:22px; height:70px;"><span style="font-family: Times-Roman; font-size:10px">1
<br>v
<br>7
<br>6
<br>5
<br>5
<br>0
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:513px; width:22px; height:5px;"><span style="font-family: Times-Roman; font-size:5px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:14px; top:518px; width:22px; height:91px;"><span style="font-family: Times-Roman; font-size:10px">0
<br>1
<br>6
<br>1
<br></span><span style="font-family: Times-Roman; font-size:5px">:
<br></span><span style="font-family: Times-Roman; font-size:10px">v
<br></span><span style="font-family: Times-Roman; font-size:5px">i
<br></span><span style="font-family: Times-Roman; font-size:14px">X
<br></span><span style="font-family: Times-Roman; font-size:6px">r
<br></span><span style="font-family: Times-Roman; font-size:8px">a
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:892px; width:612px; height:792px;"></span>
<div style="position:absolute; top:892px;"><a name="2">Page 2</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:1064px; width:108px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Contents
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:1139px; width:69px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Introduction
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1153px; width:388px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:1191px; width:189px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">1 Convolutional Neural Networks
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1205px; width:388px; height:336px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1.1 Layers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.1.1 Linear or Fully Connected . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.1.2 Activation or Non Linearity . . . . . . . . . . . . . . . . . . . . . . .
<br>Spatial Convolution . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.1.3
<br>1.1.4
<br>Spatial Pooling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.1.5 Batch Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.2 Convolutional Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.2.1 CNNs (LeNet)
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.2.2 Deep CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.2.3 Very Deep CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.2.4 Residual CNNs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.3 Training Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.3.1 From Scratch . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.3.2 Transfer Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.3.3 Loss functions
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.3.4 Optimization algorithms . . . . . . . . . . . . . . . . . . . . . . . . .
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">1.3.5 Regularization Approaches
<br>. . . . . . . . . . . . . . . . . . . . . . .
<br>Interpretability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.4.1 De&#64257;nitions
<br>1.4.2
<br>. . . . . . . . . . . . . . . . . . . .
<br>1.4.3 Advanced Visualization Techniques . . . . . . . . . . . . . . . . . . .
<br>Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.5.1 Translation invariance . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.5.2 Rotation invariance
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>Scale invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>1.5.3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:168px; top:1462px; width:154px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Simple Visualization Techniques
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1435px; width:13px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1.4
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1489px; width:13px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1.5
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:513px; top:1139px; width:6px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">1
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:509px; top:1191px; width:10px; height:349px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">5
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">5
<br>5
<br>5
<br>7
<br>8
<br>8
<br>9
<br>9
<br>10
<br>10
<br>12
<br>12
<br>12
<br>13
<br>14
<br>15
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">15
<br>16
<br>16
<br>17
<br>18
<br>19
<br>20
<br>21
<br>21
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:1562px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:1734px; width:612px; height:792px;"></span>
<div style="position:absolute; top:1734px;"><a name="3">Page 3</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:1831px; width:203px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">2 Transfer Learning for Deep CNNs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1898px; width:197px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2.2 Small dataset of objects (VOC2007)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:1844px; width:388px; height:159px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2.1 Medium dataset of food (UPMC Food101) . . . . . . . . . . . . . . . . . . .
<br>2.1.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.1.2 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.1.3 Experiments
<br>. . . . . . . . . . . . . . . . . . . . . .
<br>2.2.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.2.2 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.2.3 Experiments
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.3 Small dataset of roofs (DSG2016 online) . . . . . . . . . . . . . . . . . . . .
<br>2.3.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.3.2 Experiments
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>2.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2018px; width:173px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">3 Weakly Supervised Learning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:2031px; width:13px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">3.1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:2031px; width:388px; height:159px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.1.1 De&#64257;nition . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.1.2 Multi Instance Learning . . . . . . . . . . . . . . . . . . . . . . . . .
<br>Spatial Transformer Network . . . . . . . . . . . . . . . . . . . . . .
<br>3.1.3
<br>3.2 Applying &#64257;ne tuning to Weldon . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.2.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.2.2 Experiments
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.3 Study of Spatial Transformer Network . . . . . . . . . . . . . . . . . . . . .
<br>3.3.1 Context . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.3.2 Previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br>3.3.3 Experiments
<br>3.4 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2205px; width:60px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Conclusion
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2229px; width:63px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Appendices
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2254px; width:63px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">A Overfeat
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2278px; width:50px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">B Vgg16
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2303px; width:83px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">C InceptionV3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:2404px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:1831px; width:12px; height:173px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">23
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">23
<br>23
<br>23
<br>25
<br>28
<br>28
<br>28
<br>29
<br>30
<br>30
<br>31
<br>32
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2018px; width:12px; height:173px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">34
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">34
<br>34
<br>34
<br>35
<br>38
<br>38
<br>39
<br>39
<br>39
<br>39
<br>40
<br>42
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2205px; width:12px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">44
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2229px; width:12px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">45
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2254px; width:12px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">46
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2278px; width:12px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">47
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:507px; top:2303px; width:12px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">48
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:2576px; width:612px; height:792px;"></span>
<div style="position:absolute; top:2576px;"><a name="4">Page 4</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:281px; top:2797px; width:48px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Abstract
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:2818px; width:428px; height:187px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The goal of our research is to develop methods advancing automatic visual recognition. In
<br>order to predict the unique or multiple labels associated to an image, we study di&#64256;erent
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">kind of Deep Neural Networks architectures and methods for supervised features learning.
<br>We &#64257;rst draw up a state-of-the-art review of the Convolutional Neural Networks aiming
<br>to understand the history behind this family of statistical models, the limit of modern
<br>architectures and the novel techniques currently used to train deep CNNs. The originality
<br>of our work lies in our approach focusing on tasks with a low amount of data. We introduce
<br>di&#64256;erent models and techniques to achieve the best accuracy on several kind of datasets,
<br>such as a medium dataset of food recipes (100k images) for building a web API, or a small
<br>dataset of satellite images (6,000) for the DSG online challenge that we¡¯ve won. We also
<br>draw up the state-of-the-art in Weakly Supervised Learning, introducing di&#64256;erent kind of
<br>CNNs able to localize regions of interest. Our last contribution is a framework, build on
<br>top of Torch7, for training and testing deep models on any visual recognition tasks and on
<br>datasets of any scale.
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:3418px; width:612px; height:792px;"></span>
<div style="position:absolute; top:3418px;"><a name="5">Page 5</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:253px; top:3685px; width:104px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Acknowledgements
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:3705px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">I would speci&#64257;cally like to thank Prof. Matthieu Cord and Assoc. Prof. Nicolas Thome
<br>for supervising my research on this project and providing resources for the experiments.
<br>Additionally, I thank all the people at LIP6 for the perfect working atmosphere.
<br>Lastly, I thank my family and friends for their love and support.
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:4260px; width:612px; height:792px;"></span>
<div style="position:absolute; top:4260px;"><a name="6">Page 6</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:4432px; width:153px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Introduction
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:4505px; width:56px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:4539px; width:428px; height:159px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Since the beginning of the Web 2.0, the amount of visual data has grown exponentially.
<br>As an example, the director of Facebook AI Research Yann LeCun has said that almost
<br>1 billion new photos were uploaded each day on Facebook in 2016 </span><span style="font-family: KEWQQA+CMR8; font-size:7px">1</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Thus, computer
<br>vision has become ubiquitous in our society, with many applications such as search engine,
<br>image understanding, medicine and self-driving car. Core to many of these applications
<br>are visual recognition tasks namely image classi&#64257;cation, localization and detection. While
<br>this seems natural to humans, those tasks are di&#64259;cult due to the large number of objects
<br>in the world, the continuous set of viewpoints from which they can be viewed, the lighting
<br>in scene, color variations, background clutter, or occlusion. Sometimes those visual tasks
<br>can even be challenging for untrained humans when several classes look very similar such
<br>as in &#64257;ne grained recognition, or very di&#64256;erent such as when age, gender, version, etc. are
<br>present.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:4708px; width:428px; height:173px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It has long been the goal of computer vision researchers to have a &#64258;exible representation of
<br>the visual world in order to recognize objects in complex scenes. During the 2000¡¯s, the best
<br>accuracy was obtained using a hand-crafted model called the Bag of visual Words (BoW).
<br>In a &#64257;rst step, robust features extractors (e.g. SIFT [25]) were applied to the dataset for
<br>extracting local descriptors from the images. Then, a clustering algorithm (e.g. K-Means)
<br>was used to obtain a visual descriptor codebook. Finally, each image were assigned to
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">their own representation in a lower space thanks to a pooling step aggregating all the
<br>descriptors. Later, a classi&#64257;er (e.g. Support Vector Machine and kernel methods) was
<br>trained on top of the vectorial representation obtained using BoW. Recent developments
<br>in Deep Learning have greatly advanced the performance of these state-of-the-art visual
<br>recognition systems to the extent of sweeping aside the hand crafted models such as BoW.
<br>Nowadays a lot of products in the industry have bene&#64257;ted from the past years of research
<br>in Deep Learning. We can cite Google Photos, Flickr and Facebook, three of the world¡¯s
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:4891px; width:135px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">1</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://youtu.be/vlQomVlaNFg
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:4930px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><span style="position:absolute; border: black 1px solid; left:91px; top:4889px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:5102px; width:612px; height:792px;"></span>
<div style="position:absolute; top:5102px;"><a name="7">Page 7</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:5199px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">largest photo sharing services, that use Deep Learning technologies to e&#64259;ciently order and
<br>sort out piles of pictures </span><span style="font-family: KEWQQA+CMR8; font-size:7px">2</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, to better target advertising, or to &#64257;nd people associated to faces
<br>[45]. Startups also are using Deep Learning to build better recognition products and to
<br>revolutionize the market providing new services </span><span style="font-family: KEWQQA+CMR8; font-size:7px">3</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Furthermore, it is used to build physical
<br>products such as self-driving cars, drones and any kind of robots equipped with cameras
<br></span><span style="font-family: KEWQQA+CMR8; font-size:7px">4</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:5287px; width:428px; height:241px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Deep Learning can be summed up as a sub &#64257;eld of Machine Learning studying statical
<br>models called deep neural networks. The latter are able to learn complex and hierarchical
<br>representations from raw data, unlike hand crafted models which are made of an essential
<br>features engineering step. This scienti&#64257;c &#64257;eld has been known under a variety of names and
<br>has seen a long history of research, experiencing alternatively waves of excitement and peri-
<br>ods of oblivion [37]. Early works on Deep Learning, or rather on Cybernetics, as it used to
<br>be called back then, have been made in 1940-1960s, and describe biologically inspired mod-
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">els such as the Perceptron, Adaline, or Multi Mayer Perceptron [35, 12]. Then, a second
<br>wave called Connectionism came in the 1960s-1980s with the invention of backpropagation
<br>[36]. This algorithm persists to the present day and is currently the algorithm of choice to
<br>optimize Deep Neural Networks. A notable contribution is the Convolutional Neural Net-
<br>works (CNNs) designed, at this time, to recognize relatively simple visual patterns, such
<br>as handwritten characters [21]. Finally, the modern era of Deep Learning has started in
<br>2006 with the creation of more complex architectures [13, 2, 34, 9]. Since a breakthrough
<br>in speech and natural language processing in 2011, and also in image classi&#64257;cation during
<br>the scienti&#64257;c competition ILSVRC in 2012 [18], Deep Learning has conquered many Ma-
<br>chine Learning communities, such as Reddit, and won challenges beyond their conventional
<br>applications area </span><span style="font-family: KEWQQA+CMR8; font-size:7px">5</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:5537px; width:428px; height:159px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Especially during the last four years, Deep Learning has made a tremendous impact in
<br>computer vision reaching previously unattainable performance on many tasks such as im-
<br>age classi&#64257;cation, objects detection, object localization, object tracking, pose estimation,
<br>image segmentation or image captioning [10]. This progress have been made possible by
<br>the increase in computational resources, thanks to frameworks such as Torch7, modern
<br>GPUs implementations such as Cudnn, the increase in available annotated data, and the
<br>community-based involvement to open source codes and to share models. These facts
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">allowed for a much larger audience to acquire the expertise needed to train modern convo-
<br>lutional networks. Thus, larger and deeper architectures are trained on bigger datasets to
<br>achieve better accuracy each year. Also, already trained models have shown astonishing
<br>results when transfered on smaller datasets and evaluated on di&#64256;erent visual tasks. Hence,
<br>a lot of pretrained models are available on the web. However, CNNs still possess inherent
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:5706px; width:388px; height:43px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">2</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://googleresearch.blogspot.fr/2013/06/improving-photo-search-step-across.html
<br></span><span style="font-family: UJRPBG+CMR6; font-size:5px">3</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://blog.ventureradar.com/2016/01/19/18-deep-learning-startups-you-should-know
<br></span><span style="font-family: UJRPBG+CMR6; font-size:5px">4</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://fortune.com/2015/12/21/elon-musk-interview
<br></span><span style="font-family: UJRPBG+CMR6; font-size:5px">5</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://blog.kaggle.com/2014/04/18/winning-the-galaxy-challenge-with-convnets
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:5772px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2
<br></span></div><span style="position:absolute; border: black 1px solid; left:91px; top:5705px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:5944px; width:612px; height:792px;"></span>
<div style="position:absolute; top:5944px;"><a name="8">Page 8</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:6041px; width:428px; height:119px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">limitations. From a theoretical perspective, Deep Neural Networks are not well understood
<br>due to their non convex property. Despite numerous e&#64256;orts, a proof of convergence to good
<br>local minima has never been found. Thus, most of the research made in this &#64257;eld are ex-
<br>perimentally driven and empirical [48]. From a practical perspective, their need for large
<br>amounts of training samples does not provide them the ability to generalize when trained
<br>on small and medium datasets. In this context, weakly supervised learning methods, that
<br>we describe in the next chapter, can be applied to overcome this limitations. Nevertheless,
<br>Deep Neural Networks seems to be the most promising kind of models for solving visual
<br>recognition.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:6169px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The progress, needs and expectations of Deep Learning are undoubtedly signs of the Big
<br>Data era, where images of any kind and computable capabilities are more available than
<br>ever before.
<br>In this context, the Convolutional Neural Networks are the most e&#64259;cient
<br>statistical model for visual recognition. The goal of this work is to produce an analysis
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">of the state-of-the-art methods to train such models, to explain their limitations and to
<br>propose original idea to overcome the latter.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:6276px; width:97px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">Contributions
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:6309px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">This master¡¯s thesis introduces a number of contributions to di&#64256;erent aspects of visual
<br>recognition. However our work is focused on classifying images and recognizing objects
<br>using global labels (e.g. one label to indicates the presence or absence of the object).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:6349px; width:412px; height:73px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In the &#64257;rst chapter, we draw up the state of the art explaining how Convolutional
<br>Neural Networks (CNNs) achieve such good accuracy, describing di&#64256;erent architec-
<br>tures and clarifying their limits. In the last chapter, we also draw up the state of the
<br>art in Weakly Supervised Learning which gather methods to improve the accuracy
<br>of CNNs trained on a few amount of images.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:6423px; width:412px; height:100px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In the second chapter, we apply Deep Neural Networks on a medium dataset of food
<br>recipes. We notably show that training CNNs From Scratch with large amount of
<br>parameters is possible on this kind of datasets reaching way better accuracy than the
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">BoW model. We also show that Fine Tuning of pretrain models is essential, especially
<br>on small dataset. We illustrate this last point by presenting our winning solution of
<br>the Data Science Game Online Selection, an image classi&#64257;cation challenge made for
<br>master and PhD students from all around the world.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:6525px; width:412px; height:59px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In the last chapter, we study techniques to overcome the limited spatial invariance
<br>capacity of CNNs without the use of rich annotations such as bounding boxes. The
<br>&#64257;rst technique consists in providing such invariance directly by feeding the networks
<br>augmented images. The second consists in using a novel approach developed by a
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:6614px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">3
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:6786px; width:612px; height:792px;"></span>
<div style="position:absolute; top:6786px;"><a name="9">Page 9</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:6883px; width:401px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">PhD student at LIP6 which gives the network the ability to localize regions of interest.
<br>The third one consists in using a &#64257;rst network to localize precisely the object and a
<br>second network to classify the resulting image. Our main results are synthesized at
<br>the end of this chapter.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:6936px; width:412px; height:73px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Overall, this study has helped us to develop </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Torchnet-vision</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, a framework build
<br>on top of Torch7 that serves as a plugin for Torchnet (a high level deep learning
<br>framework) for training easily the last architectures and pretrain models on several
<br>datasets. Reproducibility of a large amount of our experiments is ensured by the fact
<br>that we provide links to our code in footnotes.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:7456px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">4
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:7628px; width:612px; height:792px;"></span>
<div style="position:absolute; top:7628px;"><a name="10">Page 10</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:7798px; width:102px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Chapter 1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:7852px; width:385px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Convolutional Neural Networks
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:7925px; width:82px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">1.1 Layers
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:7958px; width:195px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.1.1 Linear or Fully Connected
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:7986px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Mathematically, we can think of a linear layer as a function which applies a linear trans-
<br>formation on a vectorial input of dimension </span><span style="font-family: TKAABP+CMMI10; font-size:10px">I </span><span style="font-family: MTIPLH+CMR10; font-size:10px">and output a vector of dimension </span><span style="font-family: TKAABP+CMMI10; font-size:10px">O</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Usually
<br>the layer has a bias parameter.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:274px; top:8019px; width:63px; height:18px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">A </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x </span><span style="font-family: MTIPLH+CMR10; font-size:10px">+ </span><span style="font-family: TKAABP+CMMI10; font-size:10px">b
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:280px; top:8021px; width:15px; height:40px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">I</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:257px; top:8057px; width:20px; height:11px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i </span><span style="font-family: MTIPLH+CMR10; font-size:10px">=
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:296px; top:8057px; width:57px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">A</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i,j</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">j</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) + </span><span style="font-family: TKAABP+CMMI10; font-size:10px">b</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:281px; top:8072px; width:14px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">j</span><span style="font-family: KEWQQA+CMR8; font-size:7px">=1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8094px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The linear layer is motivated by the basic computational unit of the brain called neuron.
<br>Approximately 86 billion neurons can be found in the human nervous system and they are
<br>connected with approximately 10</span><span style="font-family: KEWQQA+CMR8; font-size:7px">14 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">- 10</span><span style="font-family: KEWQQA+CMR8; font-size:7px">15 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">synapses. Each neuron receives input signals from
<br>its dendrites and produces output signal along its axon. The linear layer is a simpli&#64257;cation
<br>of a group of neuron having their dendrites connected to the same inputs. Usually an
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">activation function, such as sigmoid, is used to mimic the 1-0 impulse carried away from
<br>the cell body and also to add non linearity. However we consider here that the activation
<br>function is the identity function that output real values.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8224px; width:205px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.1.2 Activation or Non Linearity
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8252px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The capacity of the neural networks to approximate any functions, especially non-convex, is
<br>directly the result of the non-linear activation functions. Every kind of activation function
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:8298px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">5
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:8470px; width:612px; height:792px;"></span>
<div style="position:absolute; top:8470px;"><a name="11">Page 11</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:192px; top:8672px; width:12px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(a)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:406px; top:8687px; width:13px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(b)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8709px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.1: A cartoon drawing of a biological neuron (a) and its mathematical model (b).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8741px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">takes a vector and performs a certain &#64257;xed point-wise operation on it. There are three
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">main activation functions.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8789px; width:358px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Sigmoid </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The Sigmoid non-linearity has the following mathematical form
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:244px; top:8800px; width:122px; height:18px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥ò</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) = 1</span><span style="font-family: TKAABP+CMMI10; font-size:10px">/</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(1 + exp</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">&#8722;</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8828px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It takes a real value and squashes it between 0 and 1. However, when the neuron¡¯s activation
<br>saturates at either tail of 0 or 1, the gradient at these regions is almost zero. Thus, the
<br>backpropagation algorithm fail at modifying its parameters and the parameters of the
<br>preceding neural layers.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8903px; width:402px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Hyperbolic Tangent </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The TanH non-linearity has the following mathematical form
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:270px; top:8914px; width:71px; height:18px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= 2</span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥ò</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(2</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8942px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It squashes a real-valued number between -1 and 1. However it has the same drawback
<br>than the sigmoid.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:8990px; width:356px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Recti&#64257;ed Linear Unit </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The ReLU has the following mathematical form
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:272px; top:9010px; width:66px; height:10px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">max</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(0</span><span style="font-family: TKAABP+CMMI10; font-size:10px">, x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:9029px; width:428px; height:92px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The ReLU has become very popular in the last few years, because it was found to greatly
<br>accelerate the convergence of stochastic gradient descent compared to the sigmoid/tanh
<br>functions due to its linear non-saturating form (e.g. a factor of 6 in [18]).
<br>In fact, it
<br>does not su&#64256;er from the vanishing or exploding gradient. An other advantage is that it
<br>involves cheap operations compared to the expensive exponentials. However, the ReLU
<br>removes all the negative informations and thus appears not suited for all datasets and
<br>architectures.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:9140px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">6
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:97px; top:8579px; width:203px; height:86px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:311px; top:8565px; width:203px; height:116px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:9312px; width:612px; height:792px;"></span>
<div style="position:absolute; top:9312px;"><a name="12">Page 12</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:181px; top:9493px; width:12px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(a)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:374px; top:9492px; width:13px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(b)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:206px; top:9515px; width:199px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.2: A sigmoid (a) and a tanh (b).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:9549px; width:157px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.1.3 Spatial Convolution
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:9577px; width:428px; height:106px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Regular Neural Networks, only made of linear and activation layers, do not scale well to
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">full images. For instance, images of size 3 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224 (3 color channels, 224 wide, 224
<br>high) would necessitate a &#64257;rst linear layer having 3 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224 + 1 = 150</span><span style="font-family: TKAABP+CMMI10; font-size:10px">, </span><span style="font-family: MTIPLH+CMR10; font-size:10px">129 parameters
<br>for a single neurone (e.g. output). Spatial convolution layers take advantage of the fact
<br>that their input (e.g.
<br>In
<br>fact, neighboring pixels should not be a&#64256;ected by their location within image. Thus, a
<br>convolutional layer learns a set of </span><span style="font-family: TKAABP+CMMI10; font-size:10px">N</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">k </span><span style="font-family: MTIPLH+CMR10; font-size:10px">&#64257;lters </span><span style="font-family: TKAABP+CMMI10; font-size:10px">F </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">f</span><span style="font-family: KEWQQA+CMR8; font-size:7px">1</span><span style="font-family: TKAABP+CMMI10; font-size:10px">, ..., f</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">N</span><span style="font-family: AVLMVN+CMMI6; font-size:5px">k </span><span style="font-family: MTIPLH+CMR10; font-size:10px">, which are convolved spatially
<br>with input image </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, to produce a set of </span><span style="font-family: TKAABP+CMMI10; font-size:10px">N</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">k </span><span style="font-family: MTIPLH+CMR10; font-size:10px">2D features maps </span><span style="font-family: TKAABP+CMMI10; font-size:10px">z</span><span style="font-family: MTIPLH+CMR10; font-size:10px">:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:204px; top:9632px; width:297px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">images or feature maps) exhibits many spatial relationships.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:280px; top:9689px; width:51px; height:20px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">z</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">k </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">f</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">k </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:9713px; width:428px; height:174px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">where </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">is the convolution operator. When the &#64257;lter correlates well with a region of the
<br>input image, the response in the corresponding feature map location is strong. Unlike
<br>conventional linear layer, weights are shared over the entire image reducing the number of
<br>parameters per response and equivariance is learned (i.e. an object shifted in the input im-
<br>age will simply shift the corresponding responses in a similar way). Also, a fully connected
<br>layer can be seen as a convolutional layer with &#64257;lter of sizes 1 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">1 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: TKAABP+CMMI10; font-size:10px">inputSize</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br>It is important to highlight that a spatial convolution is not de&#64257;ned by the spatial size
<br>of the input feature maps (e.g. wide and high), neither by the size of the output feature
<br>maps, but by the number of &#64257;lters (e.g. number of output channels), the properties of its
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">&#64257;lters (e.g. number of input channels, wide, high) and the properties of the convolution
<br>(e.g. padding, stride). Animations showing di&#64256;erent kind of convolution can be viewed on
<br>line </span><span style="font-family: KEWQQA+CMR8; font-size:7px">1</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:9895px; width:211px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">1</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://github.com/vdumoulin/conv_arithmetic
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:9982px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">7
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:125px; top:9407px; width:125px; height:79px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:318px; top:9407px; width:125px; height:79px;"></div><span style="position:absolute; border: black 1px solid; left:91px; top:9893px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:10154px; width:612px; height:792px;"></span>
<div style="position:absolute; top:10154px;"><a name="13">Page 13</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10370px; width:428px; height:32px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.3: The illustration of a spatial pooling operation in 2 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">2 regions by a stride of 2
<br>in the high direction, and 2 in the width direction, without padding.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10425px; width:130px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.1.4 Spatial Pooling
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10453px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In Convolutional Neural Networks, a pooling layer is typically present to provide invariance
<br>to slightly di&#64256;erent input images and to reduce the dimension of the feature maps (e.g.
<br>wide, high):
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:272px; top:10492px; width:66px; height:13px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">p</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">R </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">P</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">¡ô</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">R</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">z</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10514px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">where </span><span style="font-family: TKAABP+CMMI10; font-size:10px">P </span><span style="font-family: MTIPLH+CMR10; font-size:10px">is a pooling function over the region of pixels </span><span style="font-family: TKAABP+CMMI10; font-size:10px">R</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Max pooling is preferred as
<br>it avoids cancellation of negative elements and prevents blurring of the activations and
<br>gradients throughout the network since the gradient is placed in a single location during
<br>backpropagation.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10575px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The spatial pooling layer is de&#64257;ned by its aggregation function, the high and width dimen-
<br>sions of the area where it is applied, and the properties of the convolution (e.g. padding,
<br>stride).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10637px; width:163px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.1.5 Batch Normalization
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10666px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">This layer quickly became very popular mostly because it helps to converge faster [14]. It
<br>adds a normalization step (shifting inputs to zero-mean and unit variance) to make the
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">inputs of each trainable layers comparable across features. By doing this it ensures a high
<br>learning rate while keeping the network learning.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:10727px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Also it allows activations functions such as TanH and Sigmoid to not get stuck in the
<br>saturation mode (e.g. gradient equal to 0).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:10824px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">8
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:231px; top:10249px; width:149px; height:118px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:10996px; width:612px; height:792px;"></span>
<div style="position:absolute; top:10996px;"><a name="14">Page 14</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11090px; width:235px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">1.2 Convolutional Architectures
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11124px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">A lot of convolutional architectures have been developed from the 1990¡¯s. In this section,
<br>we make an inventory of the most known architectures </span><span style="font-family: KEWQQA+CMR8; font-size:7px">2</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Each one represent a step further
<br>for more advanced visual recognition.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11186px; width:128px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.2.1 CNNs (LeNet)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11214px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">LeNet-5 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">This kind of architecture is one of the &#64257;rst successful applications of CNNs. It
<br>was developed by Yann LeCun in the 1990¡¯s and was used to read zip codes and digits. This
<br>architecture, with regard to the modern ones, di&#64256;ers on many points. Thus, we will limit
<br>ourselves on the most known, LeNet-5 [22], and we will not delve into the details. In overall
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">this network was the origin of much of the recent architectures, and a true inspiration for
<br>many people in the &#64257;eld.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11302px; width:191px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">LeNet-5 features can be summarized as:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:11315px; width:412px; height:140px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">sequence of 3 layers: convolution, pooling, non-linearity,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">inputs are normalized using mean and standard deviation to accelerate training [20],
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">sparse connection matrix between layers to avoid large computational cost
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">hyperbolic tangent or sigmoid as non-linearity function,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">trainable average pooling as pooling function,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">fully connected layers as &#64257;nal classi&#64257;er,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">mean squared error as loss function.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11595px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.4: Architecture of LeNet-5, an old convolutional neural network for digits recog-
<br>nition.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:11636px; width:258px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">2</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://culurciello.github.io/tech/2016/06/04/nets.html
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:11666px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">9
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:102px; top:11467px; width:406px; height:116px;"></div><span style="position:absolute; border: black 1px solid; left:91px; top:11634px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:11838px; width:612px; height:792px;"></span>
<div style="position:absolute; top:11838px;"><a name="15">Page 15</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11934px; width:111px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.2.2 Deep CNNs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:11962px; width:428px; height:92px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It is one of the &#64257;rst work that popularized convolutional networks in computer
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">AlexNet
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">vision. AlexNet [19] was submitted to the ImageNet ILSVRC challenge of 2012 and signi&#64257;-
<br>cantly outperformed the other hand crafted models (accuracy top5 of 84% compared to the
<br>second runner-up with 74%). This network, compared to LeNet, was deeper (60 millions
<br>of parameters) and bigger (5 convolutional layers, 3 max pooling and 3 fully-connected
<br>layers). At this time, the authors provided a multi-GPUs implementation in CUDA to
<br>bypass the memory needs. It popularized:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:12056px; width:412px; height:39px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">the ReLU as non-linearity function of choice,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">the method of stacking convolutional layers plus non-linearity on top of each other
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:12098px; width:263px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">without being immediately followed by a pooling layer,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:12110px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">the method of overlapping Max Pooling, avoiding the averaging e&#64256;ects of Average
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:12131px; width:38px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Pooling.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12301px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.5: An illustration of the architecture of AlexNet. One GPU runs the layer-parts
<br>at the top of the &#64257;gure while the others runs the layer-parts at the bottom..
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12358px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It was the winner architecture of ILSVRC2013 [38] with almost
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">Overfeat or ZFNet
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">140 millions of parameters. Based on AlexNet, the size of its middle convolutional layers
<br>have been expanded. Also, the stride and &#64257;lter size on its &#64257;rst layer have been made
<br>smaller.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12434px; width:143px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.2.3 Very Deep CNNs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12462px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It was the runner-up architecture of ILSVRC2014 [40] with al-
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">VeryDeep or VggNet
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">most 140 millions of parameters. Its main contributions were to show that depth is a critical
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:12508px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">10
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:102px; top:12154px; width:406px; height:136px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:12680px; width:612px; height:792px;"></span>
<div style="position:absolute; top:12680px;"><a name="16">Page 16</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12769px; width:428px; height:73px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">component for good performance, to use much smaller 3 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">3 &#64257;lters in each convolutional
<br>layers, and also to combine them as a sequence of convolutions. The great advantage of
<br>VggNet was the insight that multiple 3 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">3 convolution in sequence can emulate the e&#64256;ect
<br>of larger receptive &#64257;elds, for examples 5 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">5 and 7 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">7. These ideas will be also used in
<br>more recent network architectures as Inception and ResNet.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:12969px; width:428px; height:32px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.6: Filter of 5 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">5 or more can be decomposed with multiple 3 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">3 convolutions
<br>such as in VGG.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13033px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">GoogLeNet or Inception </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It was the winner architecture of ILSVRC2014 [43]. Its main
<br>contribution was the development of an Inception Module that dramatically reduced the
<br>number of parameters (40 millions) [32]. Also, it eliminated a large amount of parameters
<br>by using average pooling instead of fully connected layers at the top of the convolutional
<br>layers. Further versions of the GoogLeNet has been released. The most recent architecture
<br>available is InceptionV3 [44]. Notably, it uses batch normalization.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13248px; width:428px; height:32px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.7: 1</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">1 convolutions are used to decrease the input size before 3</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">3 convolutions
<br>in order to provide more combinational power such as in GoogLeNet.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:13350px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">11
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:241px; top:12854px; width:128px; height:112px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:91px; top:13124px; width:428px; height:121px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:13522px; width:612px; height:792px;"></span>
<div style="position:absolute; top:13522px;"><a name="17">Page 17</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13766px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.8: A skip connection is used to bypass the input to the next layers such as in
<br>ResNet.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13813px; width:132px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.2.4 Residual CNNs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:143px; top:13842px; width:327px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It was the winner architecture of ILSVRC2015 [</span><span style="font-family: WCLHMO+CMBX10; font-size:10px">?</span><span style="font-family: MTIPLH+CMR10; font-size:10px">] with 152 layers.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13842px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Its main
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">ResNet
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">contribution was to use batch normalization and special skip connections for training deeper
<br>architectures. ResNet with 1000 layers can be trained with those techniques. However, it
<br>has been empirically found that ResNet usually operates on blocks of relatively low depth
<br>(</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡­</span><span style="font-family: MTIPLH+CMR10; font-size:10px">20 - 30 layers), which act in parallel, rather than serially &#64258;ow the entire length of the
<br>network [46].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13948px; width:163px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">1.3 Training Methods
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:13981px; width:120px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.3.1 From Scratch
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:196px; top:14035px; width:4px; height:7px;"><span style="font-family: KEWQQA+CMR8; font-size:7px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14009px; width:428px; height:93px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Initialization </span><span style="font-family: MTIPLH+CMR10; font-size:10px">All the network parameters are generally initialized with Layer-sequential
<br>unit-variance (LSUV) (e.g. each parameters as Gaussian random variables with mean 0 and
<br>standard deviation
<br>and biases are initialized to zero). Since the LSUV initialization
<br>works under assumption of preserving unit variance of the input, pixel intensities are given
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">after subtracting the mean and dividing by the standard deviation. More information can
<br>be found in the chapter 3 of Michael Nielsen¡¯s book [30]. In case of pretrain networks, the
<br>mean and std of the original dataset are kept.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:185px; top:14043px; width:26px; height:8px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">n</span><span style="font-family: AVLMVN+CMMI6; font-size:5px">inputs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14128px; width:428px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Loss function </span><span style="font-family: MTIPLH+CMR10; font-size:10px">To quantify the capacity of the network to approximate the ground truth
<br>labels for all training inputs, we de&#64257;ne a loss function which takes as inputs the weights,
<br>biases, and examples from the training set. For instance, the loss could be the number
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:14192px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">12
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:183px; top:13617px; width:244px; height:138px;"></div><span style="position:absolute; border: black 1px solid; left:185px; top:14043px; width:27px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:14364px; width:612px; height:792px;"></span>
<div style="position:absolute; top:14364px;"><a name="18">Page 18</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14461px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">of images correctly classi&#64257;ed. However, the most e&#64259;cient way to &#64257;nd the weights and
<br>biases, regarding the number of parameters, is to use an algorithm similar to the Stochastic
<br>Gradient Descent (SGD). In order to do so, if our chosen loss function is not smooth, we
<br>have to chose a surrogate loss (e.g. derivable function) such as Mean Square Error or Cross
<br>Entropy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14550px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Backpropagation </span><span style="font-family: MTIPLH+CMR10; font-size:10px">For each examples, we compute the prediction and its associated
<br>loss. We sum up all the loss to compute the &#64257;nal error. Then we use the backpropagation
<br>algorithm to propagate the error in order to compute the partial derivatives </span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥äE
<br>¥äb </span><span style="font-family: MTIPLH+CMR10; font-size:10px">of
<br>the cost function </span><span style="font-family: TKAABP+CMMI10; font-size:10px">E </span><span style="font-family: MTIPLH+CMR10; font-size:10px">for all weights </span><span style="font-family: TKAABP+CMMI10; font-size:10px">w </span><span style="font-family: MTIPLH+CMR10; font-size:10px">and bias </span><span style="font-family: TKAABP+CMMI10; font-size:10px">b</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. In this work, our goal is not to explain
<br>in details how works the backpropagation algorithm. We advise the curious reader to read
<br>the chapter 2 of Michael Nielsen¡¯s book [30].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:458px; top:14576px; width:47px; height:16px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥äw </span><span style="font-family: MTIPLH+CMR10; font-size:10px">and </span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥äE
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14654px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Optimization </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Once all the derivatives are computed, we update our parameters using a
<br>chosen optimization technique such as SGD. We then iterate the predication (e.g. forward
<br>pass), the backpropagation of errors (e.g. backward pass) and the optimization until con-
<br>vergence hopping to &#64257;nd a local minimum low enough to ensure good predictions. Even if
<br>the chosen surrogate loss function of a neural network is non convex, SGD works well in
<br>practice.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14757px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Grid search </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is common to explore manually the space of hyperparameters such as
<br>learning rate, weight decay, learning rate decay, amount of dropout, not to mention the
<br>architectures hyperparameters, in order to obtain the best performance in terms of both
<br>accuracy and training time.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14818px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[28] made an evaluation on the in&#64258;uence of architecture choices and optimization hyper-
<br>parameters on ImageNet. While there are very few theoretical studies, technical studies of
<br>this kind can help the reader to reduce the space of hyperparameters to explore.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14880px; width:146px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.3.2 Transfer Learning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:14909px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Features Extraction </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It consists in extracting features from the network by forwarding
<br>examples. Transformations to the examples are possible such as horizontal &#64258;ip. Then the
<br>associated features to the example are aggregated whether by averaging or stacking them.
<br>Finally, a classi&#64257;er is trained and tested on the features. Typically, the later is a Support
<br>Vector Machine with a linear kernel.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:15034px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">13
<br></span></div><span style="position:absolute; border: black 1px solid; left:458px; top:14584px; width:10px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:14584px; width:10px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:15206px; width:612px; height:792px;"></span>
<div style="position:absolute; top:15206px;"><a name="19">Page 19</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15303px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Fine Tuning </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It consists in training a pretrained network on a smaller dataset. Typically,
<br>the last fully connected layers, which can be viewed as classi&#64257;cation layers, are reset and a
<br>smaller learning rate is applied to the pretrained layers. By doing so, the goal is to adapt
<br>the features to the new dataset. More di&#64256;erent is the latter from the original dataset, more
<br>parameters/layers must be reset.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15392px; width:124px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.3.3 Loss functions
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15420px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this subsection, we present the three most used loss function to train deep neural net-
<br>works for classi&#64257;cation.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15469px; width:154px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Mean Square Error (MSE)
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">networks.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:256px; top:15469px; width:263px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It is a multi class loss formerly used to train neural
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:239px; top:15498px; width:59px; height:10px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">Loss</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x, y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) =
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:330px; top:15490px; width:41px; height:19px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">|</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">|</span><span style="font-family: KEWQQA+CMR8; font-size:7px">2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:312px; top:15463px; width:15px; height:40px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:319px; top:15514px; width:2px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:15491px; width:6px; height:25px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">n
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15528px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">with </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x </span><span style="font-family: MTIPLH+CMR10; font-size:10px">a vector of </span><span style="font-family: TKAABP+CMMI10; font-size:10px">n </span><span style="font-family: MTIPLH+CMR10; font-size:10px">predictions, and </span><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">a binary vector full of 0 besides a 1 in the corre-
<br>sponding class dimension .
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15577px; width:398px; height:10px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Cross Entropy </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is a multi class loss which is nearly a better choice than MSE.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:207px; top:15571px; width:88px; height:46px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">Loss</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x, y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) = </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722;</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:336px; top:15580px; width:15px; height:44px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:80)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:298px; top:15598px; width:37px; height:19px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">log</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:287px; top:15622px; width:2px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:350px; top:15599px; width:48px; height:28px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">exp</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span><span style="font-family: QMJINZ+CMMI8; font-size:7px">j </span><span style="font-family: TKAABP+CMMI10; font-size:10px">exp</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">j</span><span style="font-family: MTIPLH+CMR10; font-size:10px">))
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:400px; top:15606px; width:4px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15640px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">with </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x </span><span style="font-family: MTIPLH+CMR10; font-size:10px">a vector of </span><span style="font-family: TKAABP+CMMI10; font-size:10px">n </span><span style="font-family: MTIPLH+CMR10; font-size:10px">predictions, and </span><span style="font-family: TKAABP+CMMI10; font-size:10px">y </span><span style="font-family: MTIPLH+CMR10; font-size:10px">a binary vector full of 0 besides a 1 in the corre-
<br>sponding class dimension .
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15674px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In fact, it may happen that the initialization of the parameters result in the network being
<br>decisively wrong for some training input (an output neuron will have saturated near 1,
<br>when it should be 0, or vice versa). The MSE loss will usually slow down learning, but the
<br>Cross Entropy loss won¡¯t. More information can be found in the chapter 3 of [30].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:15750px; width:326px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Loss Multi Label
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">&#64257;cation. It is a multi-label one-versus-all loss based on max-entropy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:196px; top:15750px; width:323px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">It is the adaptation of the Cross Entropy loss for multi-label classi-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:139px; top:15758px; width:88px; height:46px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">Loss</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x, y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) = </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722;</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:218px; top:15809px; width:2px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:227px; top:15785px; width:41px; height:19px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">log</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:280px; top:15786px; width:34px; height:11px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">exp</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:270px; top:15801px; width:53px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1 + </span><span style="font-family: TKAABP+CMMI10; font-size:10px">exp</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:325px; top:15785px; width:82px; height:19px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">) + (1 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">log</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:433px; top:15786px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:409px; top:15801px; width:53px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">1 + </span><span style="font-family: TKAABP+CMMI10; font-size:10px">exp</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:464px; top:15793px; width:8px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">))
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:15876px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">14
<br></span></div><span style="position:absolute; border: black 1px solid; left:303px; top:15504px; width:6px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:336px; top:15612px; width:62px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:15799px; width:53px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:409px; top:15799px; width:53px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:16048px; width:612px; height:792px;"></span>
<div style="position:absolute; top:16048px;"><a name="20">Page 20</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16144px; width:184px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.3.4 Optimization algorithms
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16172px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The loss function of a CNN is highly non convex. Hopefully the latter is also fully derivable,
<br>so that gradient based optimization algorithms can be applied. However, CNNs are usually
<br>made of tens of millions of parameters. Thus, only the &#64257;rst order derivatives are used in
<br>practice. In fact, the second derivatives are costly in term of memory and computational
<br>e&#64256;ort.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16262px; width:428px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Stochastic Gradient Descent (SGD)
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is the main optimization algorithm. It con-
<br>sists in using a few examples to compute the gradient of the parameters with respect to
<br>the loss function :
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:232px; top:16294px; width:147px; height:20px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t</span><span style="font-family: KEWQQA+CMR8; font-size:7px">+1 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥ë </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¤ ¡Ô</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: AVLMVN+CMMI6; font-size:5px">t</span><span style="font-family: TKAABP+CMMI10; font-size:10px">L</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">f</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: AVLMVN+CMMI6; font-size:5px">t</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)</span><span style="font-family: TKAABP+CMMI10; font-size:10px">, y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16323px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">There is no proof of good convergence. However, this algorithm reaches good local minima
<br>in practice, even when the parameters are randomly initialized. One of the reason could
<br>be the stochastic property of this algorithm, allowing the latter to optimize di&#64256;erent loss
<br>functions and thus to get out of bad minima. The other reason could be that a lot of local
<br>minima are almost as accurate than the global minima. Answers to this question are still
<br>under active research.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16426px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Approximation of Second Order Derivatives </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Other optimization algorithms rely
<br>on more advance techniques such as momentum, second order approximation and adap-
<br>tive learning rates [42, 17]. They are known to converge faster and their parameters are
<br>sometimes easier to tune by grid search. However, they take a bit more processing time to
<br>compute, but also much more memory (2 to 3 more).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16516px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Distributed SGD </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is the kind of optimization used in parallel computing environ-
<br>ments. Di&#64256;erent computers train the same architecture with almost the same parameters
<br>values. It allows more exploration of the parameters space, which can lead to improved
<br>performance [50].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16591px; width:199px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.3.5 Regularization Approaches
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16620px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Deep and large enough neural networks can memorize any data. During training, their
<br>accuracy on the trainset typically converges towards perfection while it degrades on the
<br>testset. This phenomenon is called over&#64257;tting.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:16718px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">15
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:16890px; width:612px; height:792px;"></span>
<div style="position:absolute; top:16890px;"><a name="21">Page 21</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:16987px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Regularization L2 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The &#64257;rst main approach to overcome over&#64257;tting is the classical
<br>weight decay, which adds a term to the cost function to penalize the parameters in each
<br>dimension, preventing the network from exactly modeling the training data and therefore
<br>help generalize to new examples:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:352px; top:17012px; width:15px; height:40px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:369px; top:17045px; width:9px; height:15px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">2
<br></span><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:231px; top:17047px; width:118px; height:10px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">Err</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x, y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) = </span><span style="font-family: TKAABP+CMMI10; font-size:10px">Loss</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">x, y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) +
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17075px; width:260px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">with </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è </span><span style="font-family: MTIPLH+CMR10; font-size:10px">a vector containing all the network parameters.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:358px; top:17063px; width:2px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17110px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Data augmentation </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is a method of boosting the size of the training set so that the
<br>model cannot memorize all of it. This can take several forms depending of the dataset.
<br>For instance, if the objects are supposed to be invariant to rotation such as galaxies or
<br>planktons, it is well suited to apply di&#64256;erent kind of rotations to the original images.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17185px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Dropout </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Finally, a recent success has been shown with a regularization technique called
<br>Dropout [41]. The idea is to randomly set a certain percentage of the activations in
<br>each layers to 0. During the training, neurons must learn better representations without
<br>co-adapting to each other being active. During the testing, all the neurons are used to
<br>compute the prediction and Dropout acts like a form of model averaging over all possible
<br>instantiations of the model.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17288px; width:428px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Early stopping </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It consists in stopping the training before the model begins to over&#64257;t
<br>the training set. In practice, it is used a lot during the training of neural networks.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17339px; width:145px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">1.4 Interpretability
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17372px; width:104px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.4.1 De&#64257;nitions
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17401px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The desire for interpretation presupposes that predictions alone do not su&#64259;ce [23]. Typi-
<br>cally, we train models to achieve strong predictive power. However, this objective can be
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">a weak surrogate for the real-world goals of machine learning practitioners.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17462px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Intelligibility </span><span style="font-family: MTIPLH+CMR10; font-size:10px">There are two de&#64257;nitions of interpretability. The &#64257;rst one is linked with
<br>understandability or intelligibility, i.e., that we can grasp how the model works. Multiple
<br>criteria are used to evaluate if a model is interpretable or not. For instance: </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">will it converge?
<br>do we understand what each parameters represents? is it simple enough to be examined
<br>all at once by a human? </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Understandable models are sometimes called transparent, while
<br>incomprehensible models are called black boxes.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:17560px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">16
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:17732px; width:612px; height:792px;"></span>
<div style="position:absolute; top:17732px;"><a name="22">Page 22</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17829px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Post-hoc interpretability </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Discussions of interpretability sometimes suggest that hu-
<br>man decision-makers, despite being black boxes, are themselves interpretable because they
<br>can explain their actions. Deep learning models are also often considered as black boxes.
<br>However visualization techniques can help to generate post-hoc interpretations in order to
<br>explain their actions.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17918px; width:230px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.4.2 Simple Visualization Techniques
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:17946px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The &#64257;rst technique consists in visualizing the images directly. For instance, computing
<br>the loss over all the testing set allows to visualize the easiest or hardest examples for
<br>the network. Also, computing the activations of a certain layer to identify the k-nearest
<br>neighbors based on the proximity in the space learned by the model can be a good way to
<br>understand what the chosen layer has learned.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:18021px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">A similar approach consists to visualize high-dimensional distributed representations with
<br>t-SNE [26], a technique that renders 2D visualizations in which nearby data points are
<br>likely to appear close together.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:18326px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.9: Embedded images from ImageNet in 2D space using t-SNE and features ex-
<br>tracted from a CNN.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:18367px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">An other technique consists in visualizing the features map generated by the network.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:18402px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">17
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:183px; top:18071px; width:244px; height:244px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:18574px; width:612px; height:792px;"></span>
<div style="position:absolute; top:18574px;"><a name="23">Page 23</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:130px; top:18792px; width:351px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.10: Original images with their associated gradient based images.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:18943px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.11: The original Lena photo and the resulting deconvolutional image which added
<br>to the original could improved Lena to look more like pornography (The goal is to under-
<br>stand what a CNN has learned).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19005px; width:296px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">However, this method is not suited when the network is large.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19041px; width:248px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.4.3 Advanced Visualization Techniques
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19069px; width:428px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Gradient based </span><span style="font-family: MTIPLH+CMR10; font-size:10px">A popular approach is to render the gradient as an image [39]. While
<br>this does not say precisely how a model works, it conveys which image regions the current
<br>classi&#64257;cation depends upon most heavily.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19116px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[29] attempt to explain what a network has learned by altering the input through gradient
<br>descent to enhance the activations of certain nodes selected from the hidden layers. An
<br>inspection of the perturbed inputs can give clues to what the models has learned.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19164px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">An other technique consists in training deconvolutional neural networks [48, 49]. One
<br>example of using this powerful technique is to understand what CNNs are looking at when
<br>they see nudity </span><span style="font-family: KEWQQA+CMR8; font-size:7px">3</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:19211px; width:418px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">3</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://blog.clarifai.com/what-convolutional-neural-networks-see-at-when-they-see-nudity
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:19244px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">18
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:183px; top:18669px; width:244px; height:112px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:183px; top:18817px; width:244px; height:115px;"></div><span style="position:absolute; border: black 1px solid; left:91px; top:19209px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:19416px; width:612px; height:792px;"></span>
<div style="position:absolute; top:19416px;"><a name="24">Page 24</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19656px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.12: Detections performed by &#64257;lters of AlexNet. The &#64257;lters are speci&#64257;c to a part
<br>and they work well on several object classes containing it.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19704px; width:428px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Semantic parts localization </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Finally other approaches study whether CNNs learn se-
<br>mantic parts of object classes in their internal representation. They investigate the re-
<br>sponses of convolutional &#64257;lters and try to associate their stimuli with semantic parts.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19752px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">A recent study [8] discovered that the discriminative power of the network can be attributed
<br>to a few discriminative &#64257;lters specialized to each object class. Despite promoting the
<br>emergence of &#64257;lters learn to respond to semantic parts (and not only object class), they
<br>found that only 34 out of 123 semantic parts in PASCAL-Part dataset emerge in AlexNet.
<br>Also networks trained for image classi&#64257;cation produced the same results than those trained
<br>for objects detection or localization.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19858px; width:109px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">1.5 Invariance
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19892px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Convolutional Neural Network is a powerful model able to learn the needed invariance
<br>directly from the data. However, when the amount of labeled images (e.g. training set) is
<br>not big enough, the architectural choices can limit the capacity of the model to generalize
<br>to unknown images (e.g. testing set), or rather the opposite, to improve both the accuracy
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">and training time.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:19966px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this section, we explain how CNNs achieve translation, rotation and scale invariance,
<br>and we propose some methods to achieve better invariance. Note that the pooling regimes
<br>make convolution slightly invariant to translation, rotation and shifting. However, it is not
<br>su&#64259;cient to be invariant to large spatial transformations.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:20086px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">19
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:183px; top:19511px; width:244px; height:134px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:20258px; width:612px; height:792px;"></span>
<div style="position:absolute; top:20258px;"><a name="25">Page 25</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:20354px; width:171px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.5.1 Translation invariance
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:20382px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Pooling, combined with striding, is a common way to achieve a degree of invariance, but
<br>even without this technique, the convolution &#64257;lters and the fully-connected layers are able
<br>to learn spatial invariance [5]. We explain our thinking in the next paragraphs.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:20429px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In &#64257;gures 1.13 and 1.14, we can see an ideal network with two convolutional layers with
<br>pooling, and two fully-connected layers.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:20455px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The &#64257;rst layer &#64257;lters (which generate the green volume) detect eyes, noses and other
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:20477px; width:391px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">basic shapes (in real CNNs, &#64257;rst layer &#64257;lters match lines and very basic textures).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:20489px; width:412px; height:46px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The second layer &#64257;lters (which generate the yellow volume) detect faces, legs and
<br>other objects that are aggregations of the &#64257;rst layer &#64257;lters (real life convolution &#64257;lters
<br>may detect objects that have no meaning to humans).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:20545px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In &#64257;gure 1.13, a face is at the corner bottom left of the image (represented by two red and
<br>a magenta point). In &#64257;gure 1.14, the same face is at the corner top left of the image. The
<br>same number of activations occurs, but they occur in di&#64256;erent regions of the green and
<br>yellow volumes. Therefore, any activation point at the &#64257;rst slice of the yellow volume means
<br>that a face was detected, independently of the face location. Then the fully-connected (FC)
<br>layer is responsible to ¡±translate¡± a face and two arms to an human body. In each examples,
<br>the activation path inside the FC layer was di&#64256;erent, meaning that a correct learning at
<br>the FC layer is essential to ensure the spatial invariance property.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:125px; top:20813px; width:360px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.13: Ideal CNN with a human face at the bottom left of the image.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:20840px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In conclusion, if a CNN is trained showing faces only at one corner, during the learning
<br>process, the fully-connected layer may become insensitive to some faces in other corners.
<br>Thus, regarding the task and the dataset, it can be suitable to use data augmentation to
<br>provide the CNN more examples of the same objects in di&#64256;erent image regions. However,
<br>for small datasets in which objects can undergo strong translations in the image, some
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:20928px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">20
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:134px; top:20662px; width:342px; height:139px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:21100px; width:612px; height:792px;"></span>
<div style="position:absolute; top:21100px;"><a name="26">Page 26</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:135px; top:21345px; width:341px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.14: Ideal CNN with a human face at the top left of the image.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21380px; width:212px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">di&#64256;erent approaches are more suitable [7, 3].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21415px; width:156px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.5.2 Rotation invariance
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21443px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Di&#64256;erent kind of invariance to rotation transformation can be important to learn. The
<br>&#64257;gure 1.15 represents the rotation invariance, where the object keeps the same label re-
<br>gardless its orientation. The &#64257;gure 1.16 represents the same-equivariance, where the object
<br>and its label must keep the same orientation (its is common for segmentation tasks).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21504px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The main approach to learn rotation invariance is to use data augmentation to provide
<br>the CNN more examples of the same objects with di&#64256;erent orientation. However, CNNs
<br>will often learn multiple copies of the same &#64257;lter in di&#64256;erent orientations. Some other
<br>approaches [6] try to reduce the redundancy, in order to lower the number of parameters,
<br>and thus to reduce the risk of over&#64257;tting.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21593px; width:134px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">1.5.3 Scale invariance
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:21622px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The main approach to learn scale invariance is to use data augmentation to provide the
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">CNN more examples of the same objects with di&#64256;erent scales. It is also possible to train
<br>several CNNs specialized in each scales and combine their predictions. Finally, other
<br>approaches consist to hard code scale invariance into the CNN such as in the Inception
<br>architecture or [16].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:21770px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">21
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:134px; top:21195px; width:342px; height:139px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:21942px; width:612px; height:792px;"></span>
<div style="position:absolute; top:21942px;"><a name="27">Page 27</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:22237px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.15: Example images for the Plankton and Galaxies datasets, which are rotation
<br>invariant.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:22514px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 1.16: Example tile from the Massachusetts buildings dataset, which is same-
<br>equivariant to rotation, and corresponding labels
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:22612px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">22
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:188px; top:22087px; width:235px; height:138px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:188px; top:22373px; width:235px; height:130px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:22784px; width:612px; height:792px;"></span>
<div style="position:absolute; top:22784px;"><a name="28">Page 28</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:22954px; width:102px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Chapter 2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23008px; width:332px; height:54px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Transfer Learning for Deep
<br>CNNs
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23110px; width:339px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">2.1 Medium dataset of food (UPMC Food101)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23144px; width:88px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.1.1 Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23172px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Web API VISIIR </span><span style="font-family: MTIPLH+CMR10; font-size:10px">VIsual Seek for Interactive Image Retrieval (VISIIR) is a project
<br>aiming at exploring new methods for semantic image annotation. In this frame, we have
<br>contributed to the demo which uses a CNN to recognize food images across 101 categories
<br>from the Dataset UPMC Food101.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23248px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">UPMC Food101 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is a large multimodal dataset [47] containing about 100,000 recipes
<br>for a total of 101 food categories. Each of them are constituted by around 800 to 950 images
<br>from Google Image found using the title of the category. Because of this, this dataset may
<br>contain some noise. It is the twin dataset of ETHZ University [4].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23324px; width:125px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.1.2 Previous work
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:23352px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">CNN on UPMC Food101 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">[47] compared di&#64256;erent visual features on this dataset and
<br>found that using VGG19 as features extractor was way more accurate (40.21% top-1 accu-
<br>racy) than using the BoW model with SIFT features (23.96%). They also compared two
<br>architectures and found that a deeper architecture achieved better as features extractor
<br>than a shallower (Overfeat with 33.91%).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:23454px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">23
<br></span></div><span style="position:absolute; border: black 1px solid; left:211px; top:23221px; width:3px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:131px; top:23257px; width:3px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:181px; top:23361px; width:3px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:23626px; width:612px; height:792px;"></span>
<div style="position:absolute; top:23626px;"><a name="29">Page 29</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:24148px; width:402px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 2.1: Illustration of the 101 food categories from the UPMC Food101 dataset.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:24296px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">24
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:145px; top:23834px; width:321px; height:303px;"></div><span style="position:absolute; border: black 1px solid; left:422px; top:24157px; width:3px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:24468px; width:612px; height:792px;"></span>
<div style="position:absolute; top:24468px;"><a name="30">Page 30</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:24565px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">CNN on ETHZ Food-101 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">A very recent paper [24] claim the state of the art on two
<br>datasets of food images, UEC-256 and ETHZ Food-101. They &#64257;ne tuned GoogLeNet, a
<br>22-layer network, on these datasets and achieved a 77.4% top-1 accuracy and 93.7% top-5
<br>on ETHZ Food-101.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:24641px; width:428px; height:146px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[27] present a system aiming to recognize the contents of a meal from
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">Image to calories
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">a single image, and then predict its nutritional contents, such as calories. They tested their
<br>CNN-based method on a dataset of 75,000 images from 23 di&#64256;erent restaurants. Estimating
<br>the size of the foods, as well as their labels (2,516), requires solving segmentation and
<br>depth / volume estimation from a single image. They &#64257;rst learned a binary classi&#64257;er
<br>between food and non-food class. In order to do so, they modi&#64257;ed ETHZ Food-101 and
<br>&#64257;ne tuned GoogLeNet on the new binary dataset called ETHZ Food-101 Background. They
<br>used ETHZ Food-201 Segmented to learn segmentation, and Gfood-3d to learn depth and
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">volume. Finally they used USDA NNDB to process the amount of calorie in a certain
<br>volume of food. They tested each steps of there method separately and did not provide a
<br>end-to-end test.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:24812px; width:115px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.1.3 Experiments
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:24840px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this subsection, our goal is to &#64257;nd the most accurate supervised learning method on this
<br>kind of medium sized dataset and also to understand the capacity of Convolutional Neural
<br>Networks. We use the following experimental protocol: we train each models on the same
<br>80% of the dataset and validate the results on the remaining 20%. However, we select the
<br>models hyper parameters regarding our score in validation. This is the usual procedure for
<br>medium or big sized dataset (e.g. ImageNet). In table 2.2, we summarize our results in
<br>order to easily compare the supervised learning methods studied. Notably, our results can
<br>be reproduced using our framework </span><span style="font-family: KEWQQA+CMR8; font-size:7px">1</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:24970px; width:428px; height:92px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Forward-backward Benchmark </span><span style="font-family: MTIPLH+CMR10; font-size:10px">As told earlier, one of our main concern is to &#64257;nd the
<br>most e&#64259;cient techniques to achieve the best accuracy. Since we have multi threaded the
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">data loading and data augmentation procedure, our GPU never need to wait for the latter to
<br>&#64257;nish. Thus, we make a benchmark to compare di&#64256;erent architectures and implementations
<br>in term of speed. In this study, we use a Nvidia GTX Titan X Maxwell GPU and a Intel
<br>Xeon E5-2630 v3 (2.40GHz). We summarize our results in table 2.1. Notably, InceptionV3
<br>combined with cudnn is the fastest network. Still, it has the highest depth and number
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:25071px; width:409px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">1</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://github.com/Cadene/torchnet-deep6/blob/master/src/main/upmcfood101/inceptionv3.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:25085px; width:14px; height:8px;"><span style="font-family: JXJWXC+CMTT9; font-size:8px">lua
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:25138px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">25
<br></span></div><span style="position:absolute; border: black 1px solid; left:91px; top:25070px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:25310px; width:612px; height:792px;"></span>
<div style="position:absolute; top:25310px;"><a name="31">Page 31</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:25407px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">of modules. However, it has also the lowest amount of parameters. The code for this
<br>benchmark can be found online </span><span style="font-family: KEWQQA+CMR8; font-size:7px">2</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:112px; top:25442px; width:29px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Model
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:167px; top:25442px; width:47px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Input size
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:106px; top:25458px; width:40px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Overfeat
<br>Overfeat
<br>Overfeat
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:111px; top:25499px; width:30px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Vgg16
<br>Vgg16
<br>Vgg16
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:25540px; width:58px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">InceptionV3
<br>InceptionV3
<br>InceptionV3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:183px; top:25458px; width:16px; height:120px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">221
<br>221
<br>221
<br>224
<br>224
<br>224
<br>399
<br>399
<br>399
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:245px; top:25458px; width:38px; height:120px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">nn &#64258;oat
<br>nn cuda
<br>cudnn
<br>nn &#64258;oat
<br>nn cuda
<br>cudnn
<br>nn &#64258;oat
<br>nn cuda
<br>cudnn
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:226px; top:25442px; width:298px; height:136px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Implementation Forward (ms) Backward (ms) Total (ms)
<br>29991.49
<br>1673.14
<br>1005.63
<br>46511.26
<br>1935.98
<br>1602.96
<br>88262.84
<br>2099.83
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">978.01
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:325px; top:25458px; width:41px; height:120px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">13714.15
<br>640.36
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">115.07
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">18532.82
<br>528.00
<br>458.86
<br>31934.56
<br>787.64
<br>239.51
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:406px; top:25458px; width:41px; height:120px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">16277.34
<br>1032.78
<br>890.56
<br>27978.44
<br>1407.97
<br>1144.11
<br>56328.28
<br>1312.19
<br>738.50
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:25591px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 2.1: Benchmarks of three architectures used in our study and three implementations:
<br>nn &#64258;oat on CPUs (by Torch7), nn cuda on GPUs (by Torch7), cudnn R5 on Nvidia GPUs
<br>(by Nvidia). The forward and backward pass are averaged over 10 iterations. The measures
<br>are made in milliseconds for batches of size 50.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:25677px; width:428px; height:159px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">From Scratch better than Hand Crafted </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In table 2.2, we can see that deep models
<br>trained From Scratch, (d) and (g), achieve better accuracy than Hand Crafted models, (a)
<br>and (b). In other words, it is possible to train a deep model of 140 millions parameters
<br>on a medium dataset made of 80,000 images. This might be possible, because one image
<br>is made of 224 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8727; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">3 = 150</span><span style="font-family: TKAABP+CMMI10; font-size:10px">, </span><span style="font-family: MTIPLH+CMR10; font-size:10px">528 pixels. Thus, the trainset is made of 80,000 examples
<br>for 150,528 features. Also, we used data augmentation, dropout and early stopping as
<br>regularization techniques. We virtually increase the size of the trainset as follow. Each
<br>image is randomly rescaled between 224 and 256, then randomly cropped to scale 224, and
<br>randomly &#64258;ipped. We use SGD with Nesterov momentum as our optimization technique.
<br>Especially, we use a learning rate decay to slow down the training process after each mini
<br>batch updates. We validate the values of our hyper parameters using a unique parameters
<br>initialization (e.g. keeping the same seed).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:25861px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">From Scratch better than Features Extraction </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Deep models trained From Scratch,
<br>(d) and (g), achieve far better accuracy than models based on Features Extraction from
<br>pretrained deep models on ImageNet. Even if ImageNet contains classes and images which
<br>can be found in UPMC Food101, learning specialized features from scratch is more ac-
<br>curate. This might be possible, because the dataset is big enough and the regularization
<br>methods (e.g. data augmentation and dropout) are strong enough.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:25950px; width:371px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">2</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://github.com/Cadene/torchnet-deep6/blob/master/src/main/cnnbenchmark.lua
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:25980px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">26
<br></span></div><span style="position:absolute; border: black 1px solid; left:91px; top:25441px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25441px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25455px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25457px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25457px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25471px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25484px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25498px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25498px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25512px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25525px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25539px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25539px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25553px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:220px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:308px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:384px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:531px; top:25567px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25580px; width:439px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:208px; top:25910px; width:3px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:25948px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:26152px; width:612px; height:792px;"></span>
<div style="position:absolute; top:26152px;"><a name="32">Page 32</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:26249px; width:428px; height:105px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Fine Tuning better than From Scratch </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Fine tuned models, (e) and (h), achieve
<br>better accuracy than From Scratch models, (d) and (g). In the same manner as outlined
<br>above, it is e&#64259;cient to train a deep model on this dataset, but representations learned on
<br>ImageNet, even if less accurate, can be also useful. Thus, we experimentally chose to reset
<br>all the fully connected layers of our deep networks and keep only the parameters from the
<br>convolutional layers (14,714,688 parameters in VGG16) that we update using a 10 times
<br>slower learning rate. In this manner, the networks are able to adapt their representations
<br>to UPMC Food101 bene&#64257;ting from those learned on ImageNet.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:26379px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Very Deep better than Deep </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Overfeat, (c), (d) and (e), has almost the same num-
<br>ber of parameters than Vgg16, (f), (g) and (h), but achieve lower accuracy because the
<br>later is deeper (16 layers versus 9 in Overfeat). Thus, Vgg16 learn better transferable
<br>features than Overfeat. Also, the importance of depth has been experimentally shown on
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">ImageNet.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:26469px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">InceptionV3, the most e&#64259;cient architecture </span><span style="font-family: MTIPLH+CMR10; font-size:10px">A &#64257;ne tuned InceptionV3 reaches the
<br>highest accuracy on this dataset. Also, it is 3 times faster to converge than Vgg16, thanks
<br>to several factors. Firstly, it has the lowest amount of parameters. Secondly, it has no
<br>dropout layers. The latter helps to generalize, but slow down the learning process. Finally,
<br>it has batch normalization layers before each non linearity. The latter help to converge
<br>faster and also to generalize.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:174px; top:26561px; width:29px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Model
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:276px; top:26561px; width:221px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Test top 1 (top 5) Assoc. train top 1 (top 5)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:132px; top:26577px; width:113px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(a) Bag of visual Words
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:154px; top:26590px; width:69px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(b) BossaNova
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:127px; top:26604px; width:124px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(c) Overfeat &amp; Extraction
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:26618px; width:139px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(d) Overfeat &amp; From Scratch
<br>(e) Overfeat &amp; Fine Tuning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:132px; top:26645px; width:112px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(f) Vgg16 &amp; Extraction
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:125px; top:26659px; width:127px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(g) Vgg16 &amp; From Scratch
<br>(h) Vgg16 &amp; Fine Tuning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:26686px; width:149px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(g) InceptionV3 &amp; Fine Tuning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:306px; top:26577px; width:24px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">23.96
<br>28.59
<br>33.91
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:288px; top:26618px; width:61px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">47.46 (69.37)
<br>57.98 (78.86)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:306px; top:26645px; width:24px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">40.21
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:283px; top:26659px; width:71px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">53.62 (74.67)
<br>65.71 (82.54)
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">66.83 (84.53)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:433px; top:26577px; width:5px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">&#8211;
<br>&#8211;
<br>&#8211;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:404px; top:26618px; width:61px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">79.14 (94.49)
<br>89.69 (97.96)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:433px; top:26645px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">&#8211;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:404px; top:26659px; width:61px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">88.17 (97.68)
<br>96.18 (99.39)
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">85.34 (95.91)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:26710px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 2.2: Percentage of good classi&#64257;cation on UPMC Food101. (a), (b), (c) and (f) are
<br>reported from [47]
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:26822px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">27
<br></span></div><span style="position:absolute; border: black 1px solid; left:139px; top:26352px; width:3px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26560px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26560px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26560px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26560px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26560px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26574px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26576px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26576px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26576px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26576px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26576px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26589px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26589px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26589px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26589px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26603px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26603px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26603px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26603px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26603px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26617px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26617px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26617px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26617px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26630px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26630px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26630px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26630px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26644px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26644px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26644px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26644px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26644px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26658px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26658px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26658px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26658px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26672px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26672px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26672px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26672px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26685px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26685px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:26685px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:367px; top:26685px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:503px; top:26685px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:108px; top:26699px; width:395px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:354px; top:26719px; width:3px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:26994px; width:612px; height:792px;"></span>
<div style="position:absolute; top:26994px;"><a name="33">Page 33</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27088px; width:291px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">2.2 Small dataset of objects (VOC2007)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27121px; width:88px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.2.1 Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27150px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">PASCAL VOC 2007 dataset </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The goal of the PASCAL Visual Object Classes Chal-
<br>lenge 2007 is to recognize objects from 20 visual object classes in realistic scenes (i.e. not
<br>pre-segmented objects). It is fundamentally a multi-label supervised learning problem in
<br>that a training set of labelled images is provided (5,000 images compose the training set and
<br>5,000 images compose the testing set). The twenty object classes that have been selected
<br>are:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:27230px; width:323px; height:79px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Person: person,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Animal: bird, cat, cow, dog, horse, sheep,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Vehicle: aeroplane, bicycle, boat, bus, car, motorbike, train,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Indoor: bottle, chair, dining table, potted plant, sofa, tv/monitor
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:103px; top:27401px; width:405px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 2.2: Illustration of the 20 object classes from the PASCAL VOC2007 dataset.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27443px; width:125px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.2.2 Previous work
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27471px; width:428px; height:146px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Dense Testing </span><span style="font-family: MTIPLH+CMR10; font-size:10px">[40] evaluated the generalization capacity of their CNNs, namely VGG16
<br>and VGG19, on VOC-2007, VOC-2012, Caltech-101 and Caltech-256. They proposed a
<br>method called dense testing. The network is applied densely over the rescaled test images,
<br>i.e. the fully-connected layers are &#64257;rst converted to convolutional layers (the &#64257;rst FC layer
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">to a 7 x 7 convolutional layer, the last two FC layers to 1 x 1 convolutional layer). The
<br>resulting fully-convolutional net is then applied to the whole (uncropped) image (see &#64257;gure
<br>2.3). The result is a class score map with the number of channels equal to the number of
<br>classes. Then, to obtain a &#64257;xed-size vector of class scores for the image, the class score
<br>map is spatially averaged (Average Pooling). The test set is also augmented by horizontal
<br>&#64258;ipping of the images. Finally, the soft-max class posteriors of the original and &#64258;ipped
<br>images are averaged to obtain the &#64257;nal scores for the image.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:27664px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">28
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:93px; top:27322px; width:424px; height:68px;"></div><span style="position:absolute; border: black 1px solid; left:418px; top:27410px; width:3px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:27836px; width:612px; height:792px;"></span>
<div style="position:absolute; top:27836px;"><a name="34">Page 34</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:27933px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">They combined three CNNs &#64257;ne tuned at di&#64256;erent scales on ImageNet and achieved an
<br>impressive score of 89.7 mean AP on VOC-2007 and 89.3 on VOC-2012. Using the same
<br>amount of data, the highest score reported at this time was 82.4 on VOC-2007 and 83.2
<br>on VOC-2012.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:140px; top:28130px; width:116px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(a) Convolutional Network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:341px; top:28132px; width:142px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(b) Fully Convolutional Network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28155px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 2.3: Illustration of a possible modi&#64257;cation of VGG16 to a fully convolutional archi-
<br>tecture in order to process bigger image than of size 3x224x224.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28213px; width:115px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.2.3 Experiments
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28241px; width:428px; height:132px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Fine tuning outperforms the rest </span><span style="font-family: MTIPLH+CMR10; font-size:10px">As can bee seen in table 2.3, deep features out-
<br>perform hand-crafted methods and &#64257;ne tuning Vgg16 pretrained on ImageNet achieves
<br>the best accuracy. However the dataset is too small to &#64257;t Vgg16 From Scratch. In this
<br>experiment, we train our networks on multiple scales varying from 224 to 256 with ran-
<br>dom horizontal &#64258;ips, so that the forwarded cropped image is of size 224 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">224. Then, we
<br>test our models on single scale images (224). In (e), we reset the last two fully connected
<br>layers and we train the others with a 10 times smaller learning rate. In (d), we extract
<br>deep features from the last ReLU non linearity (before the last fully connected layer) and
<br>then we train support vector machines in a one-versus-all strategy cross-validating the
<br>regularization parameter.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:28506px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">29
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:91px; top:27997px; width:214px; height:126px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:311px; top:27994px; width:203px; height:131px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:28678px; width:612px; height:792px;"></span>
<div style="position:absolute; top:28678px;"><a name="35">Page 35</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:215px; top:28774px; width:54px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Model type
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:221px; top:28790px; width:41px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(a) BoW
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:163px; top:28803px; width:159px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(b) BossaNova and FishersVector
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:187px; top:28817px; width:110px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(c) Vgg16 from scratch
<br>(d) Vgg16 extraction
<br>(e) Vgg16 &#64257;ne tuned
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:334px; top:28774px; width:114px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Test mAP Train mAP
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:344px; top:28790px; width:28px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">53.2
<br>61.6
<br>39.79
<br>83.22
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">85.70
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:419px; top:28790px; width:5px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">&#8211;
<br>&#8211;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:409px; top:28817px; width:24px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">99.73
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:419px; top:28831px; width:5px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">&#8211;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:409px; top:28844px; width:24px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">98.81
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28868px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 2.3: Comparison between hand crafted features models (a,b) and deep features
<br>models (c,d,e) tested on a single scale (224). (a) and (b) are reported from [1].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28924px; width:322px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">2.3 Small dataset of roofs (DSG2016 online)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:28958px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this section, we show that &#64257;ne tuning an InceptionV3 architecture is still the most
<br>powerful technique for this kind of dataset. However, we use a bootstrap method to reduce
<br>over&#64257;tting and thus to achieve the best accuracy on this dataset amongst more than 110
<br>teams.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29034px; width:88px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.3.1 Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29060px; width:428px; height:66px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Data Science Game </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The DSG </span><span style="font-family: KEWQQA+CMR8; font-size:7px">3 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">is an international student challenge in Machine Learn-
<br>ing. The &#64257;rst phase of this challenge is an online selection. It took place on Kaggle from
<br>the 14th June 2016 until the 10th July 2016 gathering more than 110 teams from all around
<br>the world competing for the &#64257;rst 20 positions. The &#64257;nal phase will took place during the
<br>week end of the 10th September in the castle of Cap Gemini.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29136px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">The team (Jonquille UPMC) that we represented reached the &#64257;rst position of the online
<br>selection </span><span style="font-family: KEWQQA+CMR8; font-size:7px">4</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. In this section, we will explain our method in details.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29186px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Dataset
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is a small dataset of satellite images of roofs. The goal is to predict the
<br>orientation of the roofs into 4 di&#64256;erent categories. Models are evaluated with the multiclass
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">accuracy top1 metric. The public leaderboard is made of 40% of the testing set. The private
<br>leaderboard is made of 60% others. The dataset is composed by:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:29239px; width:412px; height:46px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">8000 images in the training set, 3479 of which are in the category </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">North-South ori-
<br>entation</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 1856 in the category </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">East-West orientation</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 859 in the category </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Flat roof
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">and 1806 in the category </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Other</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:29294px; width:383px; height:21px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">3</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://www.datasciencegame.com
<br></span><span style="font-family: UJRPBG+CMR6; font-size:5px">4</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://inclass.kaggle.com/c/data-science-game-2016-online-selection/leaderboard
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:29348px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">30
<br></span></div><span style="position:absolute; border: black 1px solid; left:157px; top:28773px; width:297px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28773px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28773px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28773px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28773px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28787px; width:297px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28789px; width:297px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28789px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28789px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28789px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28789px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28802px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28802px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28802px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28802px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28816px; width:297px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28816px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28816px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28816px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28816px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28830px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28830px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28830px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28830px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28843px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:328px; top:28843px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:388px; top:28843px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:454px; top:28843px; width:0px; height:13px;"></span>
<span style="position:absolute; border: black 1px solid; left:157px; top:28857px; width:297px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:29292px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:29520px; width:612px; height:792px;"></span>
<div style="position:absolute; top:29520px;"><a name="36">Page 36</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:29690px; width:70px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(a) North-South
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:222px; top:29697px; width:61px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(b) East-West
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:338px; top:29698px; width:33px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(c) Flat
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:436px; top:29690px; width:42px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(d) Other
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:152px; top:29720px; width:307px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 2.4: Illustrations of the DSG2016 online selection dataset
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:29747px; width:280px; height:39px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">20,760 images in the training set, but without any label,
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">13,999 images in the testing set.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29811px; width:428px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Rules </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Pretrain models are usable only if they are publicly available and if they are not
<br>trained on roof datasets.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29860px; width:115px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">2.3.2 Experiments
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29888px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Winning solution </span><span style="font-family: MTIPLH+CMR10; font-size:10px">One of the main di&#64259;culties of this challenge was the relatively small
<br>size of the dataset. Yet, we thought that the latter was big enough to &#64257;ne-tune a pretrained
<br>model on. Our very &#64257;rst submission was a VGG16 &#64257;netuned on 80% of the trainset with
<br>some data augmentation and validated on the rest. It scored a whooping 82.67% on the
<br>public leaderboard (and 81.37% on the private), which put us on the top 3 at the time,
<br>and con&#64257;rmed it was working.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:29976px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">We then tried InceptionV3 because of its good performance in many transfer learning
<br>setups. In order to reduce over&#64257;tting, we used the following techniques: online data aug-
<br>mentation, the 90 trick (multiplying by 2 the number of images and moving the rotated
<br>images from class ¡±north-south¡± to class ¡±east-west¡± and vice-versa) and early stopping.
<br>We didn¡¯t add more features (images height / weight) because it seemed to lead to more
<br>over&#64257;tting, which was a main concern.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30064px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Still, it was di&#64259;cult to evaluate our models on the little test data that remained. There
<br>was a lot of variance between di&#64256;erent epochs, folds and hyperparameters. To gain more
<br>stability and to use the full training set, we went to the bagging method. We &#64257;ne tuned some
<br>InceptionV3 with di&#64256;erent hyperparameters on random bootstraps. The &#64257;nal prediction is
<br>the result of a vote among all classi&#64257;ers.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30139px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">We also tweaked the predictions of our &#64257;nal submission to counteract the fact that the
<br>train set had unbalanced classes, while the test set was balanced. This improved a bit our
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:30190px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">31
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:117px; top:29622px; width:73px; height:61px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:216px; top:29616px; width:73px; height:74px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:318px; top:29615px; width:73px; height:77px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:420px; top:29622px; width:73px; height:61px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:30362px; width:612px; height:792px;"></span>
<div style="position:absolute; top:30362px;"><a name="37">Page 37</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30459px; width:26px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">score.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30494px; width:428px; height:92px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Other solutions </span><span style="font-family: MTIPLH+CMR10; font-size:10px">We &#64257;ne tuned in a end-to-end manner a Spatial Transformer (STN)
<br>architecture with a &#64257;rst InceptionV3 to localize a region of interest and a second Incep-
<br>tionV3 to classify this region. We tried to constraint the kind of spatial transformation the
<br>network could achieve, or the number of parameters to tune. However, we think that the
<br>localizer learned the identity transformation. Also, it was expensive to train this network
<br>(twice the number of parameters compared to a simple InceptionV3) and over&#64257;tting was a
<br>main concern, thus we stopped to explore this solution.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30611px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Second team solution </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The team ranked second used a semi-supervised technique to
<br>augment their training set with unlabeled images. They also used a stacking of 84 models:
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">12 root models trained on 7 di&#64256;erent versions of the images </span><span style="font-family: KEWQQA+CMR8; font-size:7px">5</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. However they seemed to have
<br>over&#64257;tted too much the public testing set regarding the gap with their private score.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:118px; top:30676px; width:80px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Fine tuned models
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:221px; top:30676px; width:42px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px"># Models
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:142px; top:30691px; width:32px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">VGG16
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:132px; top:30703px; width:53px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">InceptionV3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:115px; top:30715px; width:87px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">InceptionV3 + STN
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:30727px; width:101px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">VGG19 (Second Team)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:132px; top:30739px; width:53px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">InceptionV3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:30751px; width:89px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">InceptionV3 + Prior
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:238px; top:30691px; width:9px; height:69px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">1
<br>1
<br>1
<br>84
<br>91
<br>91
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:318px; top:30676px; width:21px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Data
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:30691px; width:51px; height:33px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">80%trainset
<br>80%trainset
<br>80%trainset
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:276px; top:30727px; width:105px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">100%trainset + no label
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:30739px; width:56px; height:21px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">100%trainset
<br>100%trainset
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:393px; top:30676px; width:109px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Public (%) Private (%)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:404px; top:30691px; width:26px; height:69px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">82.67
<br>83.69
<br>84.73
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:9px">87.60
<br></span><span style="font-family: MTIPLH+CMR10; font-size:9px">87.32
<br>87.52
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:465px; top:30691px; width:26px; height:69px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">81.37
<br>82.61
<br>84.11
<br>86.38
<br>86.58
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:9px">86.76
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:103px; top:30773px; width:404px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 2.4: Summary of the di&#64256;erent models submitted for the DSG online challenge.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30816px; width:113px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">2.4 Conclusion
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30850px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this chapter, we studied the transfer capacity of the latest convolutional architectures.
<br>To do so, we compared three training approaches on several datasets which all have their
<br>own particularity : size, semantic distance from ImageNet, geometric variability of their
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">regions of interest.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:30911px; width:262px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Recall that our approaches can be synthesize as follow:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:30923px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">We trained speci&#64257;c deep convolutional networks randomly initializing their parame-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:30944px; width:118px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">ters (e.g. From Scratch).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:30957px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Further, we used pre-trained networks on ImageNet to extract features from the
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:30978px; width:328px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">target dataset and trained a linear model (e.g. Features Extraction).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:30999px; width:466px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">5</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">https://medium.com/@Zelros/how-deep-learning-solved-phase-1-of-the-data-science-game-2712b949963f
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:31032px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">32
<br></span></div><span style="position:absolute; border: black 1px solid; left:102px; top:30676px; width:407px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30676px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30688px; width:407px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30690px; width:407px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30690px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30702px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30714px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30726px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30738px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:215px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:270px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:387px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:447px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:509px; top:30750px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:102px; top:30762px; width:407px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:91px; top:30997px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:31204px; width:612px; height:792px;"></span>
<div style="position:absolute; top:31204px;"><a name="38">Page 38</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31293px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Finally, we &#64257;ne tuned previous pre-trained networks to adapt their learned represen-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:31314px; width:230px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">tations to the target dataset (e.g. Fine Tuning).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:31335px; width:223px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Each datasets represent a di&#64256;erent challenge.
<br>trends:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:325px; top:31335px; width:194px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In term of accuracy, we illustrated few
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31360px; width:413px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Fine Tuning achieve the best accuracy on medium datasets, and From Scratch achieves
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:31382px; width:200px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">better accuracy than Features Extraction.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31394px; width:412px; height:46px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Fine Tuning still achieve the best accuracy on small datasets, but From Scratch
<br>achieves lower accuracy than Features Extraction. Also, approaches based on a bag-
<br>ging of models are e&#64256;ective to achieve better accuracy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:31450px; width:279px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In term of learning ability, we illustrated few other trends:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31462px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">InceptionV3 is the most accurate architecture and its batch normalization layers
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:31484px; width:115px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">allow to converge faster.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31496px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Adam is useful to reduce the number of experiments needed to &#64257;nd the optimal set
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:31517px; width:98px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">of hyper parameters.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:31530px; width:255px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Early stopping is a good way to control over&#64257;tting.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:31874px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">33
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:32046px; width:612px; height:792px;"></span>
<div style="position:absolute; top:32046px;"><a name="39">Page 39</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32216px; width:102px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Chapter 3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32270px; width:348px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Weakly Supervised Learning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32343px; width:20px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">3.1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:128px; top:32343px; width:88px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">Introduction
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32376px; width:99px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.1.1 De&#64257;nition
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32404px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">We de&#64257;ne Weakly Supervised Learning (WSL) as a machine learning framework where
<br>the model is trained using examples that are only partially annotated or labeled. For
<br>instance, an object detector is typically trained on large collection of images manually
<br>annotated with masks or bounding boxes denoting the locations of objects of interest in
<br>each image. The reliance on time-consuming human labeling poses a signi&#64257;cant limitation
<br>to the practical application of these methods. Moreover, manually annotation may not be
<br>optimal for the &#64257;nal prediction task. Thus, WSL aims at reducing the amount of human
<br>intervention needed.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32519px; width:428px; height:92px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Despite their excellent performances, current CNN architectures only carry limited invari-
<br>ance properties. Recently, attempts have been made to overcome this limitation using
<br>WSL. Many di&#64256;erent kind of WSL techniques exists in the literature according to the
<br>type of labeled data used, latent variables learned and evaluation methods selected. We
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">also consider that certain papers on multiple instance learning, attention based models
<br>or &#64257;ne grained classi&#64257;cation belong to a larger WSL framework. Below, we illustrate our
<br>thinking.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32636px; width:183px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.1.2 Multi Instance Learning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32664px; width:428px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Weakly Supervised Learning (WSL) with Max Pooling </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In computer vision, the
<br>dominant approach for WSL is the Multiple Instance Learning (MIL) paradigm. An image
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:32716px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">34
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:32888px; width:612px; height:792px;"></span>
<div style="position:absolute; top:32888px;"><a name="40">Page 40</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:32985px; width:428px; height:119px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">is considered as a bag of regions, and the model seeks the max scoring instance in each
<br>bag. In [31], the authors &#64257;xed the weights of a Vgg16 network and learned the last fully
<br>connected layer (e.g. 1x1 convolutional layer). They applied Vgg16 to images of bigger
<br>size than the original input (224x224). The resulting class score map is spatially reduced
<br>using a Max Pooling. They proposed two methods to learn scale invariance. The &#64257;rst
<br>was to rescale randomly the input image. The second was to train three specialized CNNs
<br>at three di&#64256;erent scales and then to average their class scores. The two methods lead to
<br>almost the same results. They achieved 86.3 mean AP on VOC-2012. To compare, the
<br>same CNN used as a classical features extractor achieved 78.7 mean AP.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:33253px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.1: Illustration of a Weakly Supervised Learning approach which learn to focus on
<br>a region of interest from a global label.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:33311px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In [7], the authors proposed to use
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">WSL with k Max Min Pooling (WELDON)
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">the same method than in [31], but the class score map is spatially reduced using a sum of
<br>k-Min Pooling and k-Max Pooling. As a result, it extends the selection of a single region
<br>to multiple high score regions incorporating also the low score regions, i.e. the negative
<br>evidences of a class appearance. They also proposed a ranking loss to optimize average
<br>precision. Using an average of 7 specialized CNNs at 7 di&#64256;erent scales, they achieved 90.2
<br>mean AP on VOC-2007 and 88.5 on VOC-2012. They also claim the state of the art on 5
<br>other datasets. See &#64257;gure 3.2.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:33441px; width:213px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.1.3 Spatial Transformer Network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:33469px; width:428px; height:65px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Principles </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The authors of [15] introduce a new learnable module, the Spatial Trans-
<br>former, which explicitly allows the spatial manipulation of data within the network. This
<br>di&#64256;erentiable module can be viewed as a localization network which generate parameters
<br>for a grid generator. The grid is then used by a sampler to generate a certain transforma-
<br>tion. The latter can be applied on the initial image or on the same feature map used as
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:33558px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">35
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:220px; top:33114px; width:171px; height:128px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:33730px; width:612px; height:792px;"></span>
<div style="position:absolute; top:33730px;"><a name="41">Page 41</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:33934px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.2: Illustration of the WELDON approach, a CNN trained in a weakly supervised
<br>manner to perform classi&#64257;cation or ranking.
<br>It automatically selects multiple positive
<br>(green) or negative (red) evidences on several regions in the image.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34125px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.3: Spatial Transformer Module. The input feature map U is passed to a localiza-
<br>tion network which regresses the transformation parameters </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. The regular spatial grid </span><span style="font-family: TKAABP+CMMI10; font-size:10px">G
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">over </span><span style="font-family: TKAABP+CMMI10; font-size:10px">V </span><span style="font-family: MTIPLH+CMR10; font-size:10px">is transformed to the sampling grid </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">T</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">G</span><span style="font-family: MTIPLH+CMR10; font-size:10px">), which is applied to </span><span style="font-family: TKAABP+CMMI10; font-size:10px">U </span><span style="font-family: MTIPLH+CMR10; font-size:10px">, producting the
<br>warped output feature map </span><span style="font-family: TKAABP+CMMI10; font-size:10px">V </span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34201px; width:258px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">input to the localization network such as in &#64257;gure 3.3.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34228px; width:428px; height:33px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Transformations </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Multiple transformation can be learned. For instance, </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">T</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">G</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) can ap-
<br>ply a 2D a&#64259;ne transformation </span><span style="font-family: TKAABP+CMMI10; font-size:10px">A</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. In this case, the pointwise transformation is
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34323px; width:428px; height:32px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">However the class of transformations </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">T</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è </span><span style="font-family: MTIPLH+CMR10; font-size:10px">may be more constrained, such as that used for
<br>attention
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:191px; top:34246px; width:8px; height:40px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:19)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:163px; top:34246px; width:23px; height:44px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:18) </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">s
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:176px; top:34285px; width:9px; height:21px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">s
<br>i
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:202px; top:34279px; width:65px; height:20px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">T</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">G</span><span style="font-family: MTIPLH+CMR10; font-size:10px">) = </span><span style="font-family: TKAABP+CMMI10; font-size:10px">A</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:402px; top:34240px; width:31px; height:60px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:21)&#63723;&#63725; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:424px; top:34279px; width:8px; height:32px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t
<br>i
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:439px; top:34240px; width:9px; height:60px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">&#63734;&#63736;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:359px; top:34280px; width:13px; height:25px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">12
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">22
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:383px; top:34280px; width:13px; height:25px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">13
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">23
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:324px; top:34246px; width:24px; height:45px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:20) </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">11
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:285px; top:34279px; width:8px; height:32px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">i
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">y</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t
<br>i
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:271px; top:34240px; width:68px; height:127px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">&#63734;&#63736; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">=
<br></span><span style="font-family: NTSPKR+CMEX10; font-size:40px">&#63723;&#63725; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">x</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">t
<br></span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:20) </span><span style="font-family: TKAABP+CMMI10; font-size:10px">s </span><span style="font-family: MTIPLH+CMR10; font-size:10px">0 </span><span style="font-family: TKAABP+CMMI10; font-size:10px">t</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">x
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:335px; top:34294px; width:13px; height:11px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">¥è</span><span style="font-family: KEWQQA+CMR8; font-size:7px">21
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:344px; top:34321px; width:5px; height:40px;"><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:21)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:261px; top:34362px; width:24px; height:12px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">A</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">¥è </span><span style="font-family: MTIPLH+CMR10; font-size:10px">=
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:299px; top:34368px; width:20px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">0 </span><span style="font-family: TKAABP+CMMI10; font-size:10px">s
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:330px; top:34368px; width:8px; height:11px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">t</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">y
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:34400px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">36
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:177px; top:33825px; width:257px; height:98px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:123px; top:33986px; width:364px; height:128px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:34572px; width:612px; height:792px;"></span>
<div style="position:absolute; top:34572px;"><a name="42">Page 42</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34669px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">allowing cropping, translation, and isotropic scaling by varying </span><span style="font-family: TKAABP+CMMI10; font-size:10px">s</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, </span><span style="font-family: TKAABP+CMMI10; font-size:10px">t</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">x</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, and </span><span style="font-family: TKAABP+CMMI10; font-size:10px">t</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">y</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Those pa-
<br>rameters can be learned or, to constrain even more, &#64257;xed.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34718px; width:428px; height:119px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Tra&#64259;c signs classi&#64257;cation </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The &#64257;rst example of application is the supervised classi-
<br>&#64257;cation of a tra&#64259;c signs dataset (GTSRB </span><span style="font-family: KEWQQA+CMR8; font-size:7px">1</span><span style="font-family: MTIPLH+CMR10; font-size:10px">). This kind of data is a&#64256;ected by contrast
<br>variation, rotational and translational changes. [11] used a spatial transformer network to
<br>make classi&#64257;cation more robust and accurate. They used a modi&#64257;ed version of GoogLeNet
<br>with batch normalization and added four di&#64256;erent Spatial Transformer module made of a
<br>convolutional network as localizer. This method has several advantages over existing state
<br>of the art methods in terms of performance, scalability and memory requirement. Also,
<br>they reported a lower accuracy for their architecture without spatial transformer modules
<br>(99.57% against 99.81%). An implementation is available on the Torch7 blog </span><span style="font-family: KEWQQA+CMR8; font-size:7px">2</span><span style="font-family: MTIPLH+CMR10; font-size:10px">.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:113px; top:34914px; width:384px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.4: The use of a Spatial Transformer Module on a GTSRB data sample.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:34956px; width:428px; height:187px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Fine grained bird classi&#64257;cation </span><span style="font-family: MTIPLH+CMR10; font-size:10px">The second example is the &#64257;ne grained classi&#64257;cation
<br>of a bird dataset (CUB-200-2011). The birds appear at a range of scales and orientations,
<br>are not tightly cropped, and require detailed texture and shape analysis to distinguish.
<br>This dataset contains 6k training images, 5.8k test images and covers 200 species of birds.
<br>They &#64257;rst claimed the state of the art accuracy of 82.3% with an Inception architecture
<br>pre-trained on ImageNet and &#64257;ne tuned on CUB (previous best was 81.0%). Then, they
<br>trained a spatial transformer network containing 4 parallel spatial transformer modules
<br>parameterised for attention and acting on the input image. They achieved an accuracy
<br>of 84.1% outperforming their baseline by 1.8%. The resuling output from the spatial
<br>transformers for the classi&#64257;cation network is somewhat pose-normalised representation of
<br>a bird. It was able to discover and learn part detectors in a data-driven manner without
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">any additional supervision. Also, the use of spatial transformers allowed them to use
<br>448px resolution input images without any import in performance as the output of the
<br>transformed 448px images are downsampled to 224px before being processed.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:35152px; width:277px; height:21px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">1</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://benchmark.ini.rub.de/?section=gtsrb&amp;subsection=news
<br></span><span style="font-family: UJRPBG+CMR6; font-size:5px">2</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://torch.ch/blog/2015/09/07/spatial_transformers.html
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:35242px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">37
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:198px; top:34850px; width:214px; height:53px;"></div><span style="position:absolute; border: black 1px solid; left:91px; top:35150px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:35414px; width:612px; height:792px;"></span>
<div style="position:absolute; top:35414px;"><a name="43">Page 43</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:35635px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.5: Left: the accuracy on CUB-200-2011 bird classi&#64257;cation dataset. Spatial trans-
<br>former networks with two spatial transformers modules (2 x ST-CNN) and four (4 x ST-
<br>CNN) in parallel achieve higher accuracy. Right: the transformation predicted by the 2 x
<br>ST-CNN (top row) and 4 x ST-CNN (bottom row) on the input image. Notably, for the
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">2 x ST-CNN, one of the module (shown in red) learned to detect heads, while the other
<br>(shown in green) detects the body.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:35735px; width:261px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">3.2 Applying &#64257;ne tuning to Weldon
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:35769px; width:88px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.2.1 Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:35795px; width:428px; height:80px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">MIT67 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">It is the database of the Indoor scene recognition challenge </span><span style="font-family: KEWQQA+CMR8; font-size:7px">3</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. It was released
<br>during the CVPR 2009 [33]. It contains 67 categories. 5360 images compose the training
<br>set. 1340 images compose the testing set. The number of images does not vary across cat-
<br>egories. This dataset is quite similar to Pascal Voc in term of size and di&#64259;culty, e.g. while
<br>some indoor scenes (e.g. corridors) can be well characterized by global spatial properties,
<br>others (e.g., bookstores) are better characterized by the objects they contain.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:125px; top:36004px; width:361px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.6: Illustration of the 67 indoor categories from the MIT67 dataset.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:104px; top:36031px; width:206px; height:10px;"><span style="font-family: UJRPBG+CMR6; font-size:5px">3</span><span style="font-family: JXJWXC+CMTT9; font-size:8px">http://web.mit.edu/torralba/www/indoor.html
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:36084px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">38
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:102px; top:35509px; width:407px; height:115px;"></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:91px; top:35888px; width:428px; height:104px;"></div><span style="position:absolute; border: black 1px solid; left:91px; top:36029px; width:171px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:36256px; width:612px; height:792px;"></span>
<div style="position:absolute; top:36256px;"><a name="44">Page 44</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36352px; width:115px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.2.2 Experiments
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36380px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Fine tuning a Weldon architecture </span><span style="font-family: MTIPLH+CMR10; font-size:10px">We reproduce the results of [7] and show in table
<br>3.1 that Fine Tuning the Weldon architecture at three di&#64256;erent scales improves the overall
<br>accuracy. However, the process of Fine Tuning is expensive, thus we do not provide results
<br>for bigger scale.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:148px; top:36445px; width:32px; height:60px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Model
<br>Vgg16
<br>Weldon
<br>Weldon
<br>Weldon
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:192px; top:36445px; width:145px; height:60px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Image size L6 size Report (*)
<br>224 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">224
<br>249 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">249
<br>280 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">280
<br>320 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">320
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:253px; top:36452px; width:22px; height:53px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">1 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">1
<br>2 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">2
<br>3 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">3
<br>4 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">4
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:303px; top:36460px; width:22px; height:46px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">69.93
<br>72.16
<br>72.98
<br>73.40
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:352px; top:36445px; width:111px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Extraction Fine Tuning
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:364px; top:36460px; width:22px; height:46px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">70.30
<br>72.01
<br>73.80
<br>73.96
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:425px; top:36460px; width:22px; height:46px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">70.60
<br>73.4
<br>74.03
<br>74.1
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36519px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 3.1: Accuracy top 1 on MIT67 test set. Without data augmentation and dropout.
<br></span><span style="font-family: TKAABP+CMMI10; font-size:10px">k </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= 1 aggregation (one region min, one region max). L6 size represents the number of
<br>regions (e.g.
<br>instances) evaluated during the aggregation. For size 320, 16 regions are
<br>evaluated. Results from column Report (*) are reported from [7].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36602px; width:308px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">3.3 Study of Spatial Transformer Network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36635px; width:88px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.3.1 Context
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36663px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Database </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In this section, we study the ability of the Spatial Transformer Network to
<br>learn large spatial invariance. In order to do so we create a special dataset using the original
<br>MNIST dataset padded with 2 black pixels (e.g. all our images are of scale 32</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">32 instead
<br>of the original 28 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">28). Then, we create a Translated MNIST dataset. All the images are
<br>inserted on a 100 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">100 background full of black pixels, and undergo spatial shifts on the
<br>x and y axes. Those shifts are randomly picked up between 0 and 68 (=100-32).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36766px; width:125px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.3.2 Previous work
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:36794px; width:428px; height:93px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Co-localization </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In the appendix A.2 of [15], the authors explored the use of spatial
<br>transformers in a co-localization scenario. Given a set of images that are assumed to
<br>contain instances of a common but unknown object class, the model learn from the images
<br>only to localize (with a bounding box) the common object. To achieve this, they adopted
<br>the supervision that the distance between the image crop corresponding to two correctly
<br>localized objects is smaller than to a randomly sampled image crop, in some embedding
<br>space. For a dataset </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">I </span><span style="font-family: MTIPLH+CMR10; font-size:10px">= </span><span style="font-family: TKAABP+CMMI10; font-size:10px">I</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">n </span><span style="font-family: MTIPLH+CMR10; font-size:10px">of </span><span style="font-family: TKAABP+CMMI10; font-size:10px">N </span><span style="font-family: MTIPLH+CMR10; font-size:10px">images, this translates to a triplet loss, where they
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:36926px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">39
<br></span></div><span style="position:absolute; border: black 1px solid; left:142px; top:36445px; width:327px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:186px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:243px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:285px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:344px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:346px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:404px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:36445px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36457px; width:327px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36459px; width:327px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:186px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:243px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:285px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:344px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:346px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:404px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:36459px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36471px; width:327px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:186px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:243px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:285px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:344px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:346px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:404px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:36472px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:186px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:243px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:285px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:344px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:346px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:404px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:36484px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:186px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:243px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:285px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:344px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:346px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:404px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:469px; top:36496px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:142px; top:36508px; width:327px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:37098px; width:612px; height:792px;"></span>
<div style="position:absolute; top:37098px;"><a name="45">Page 45</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37195px; width:117px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">minimized the hinge loss
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:160px; top:37193px; width:15px; height:40px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">N</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:180px; top:37193px; width:15px; height:40px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">M</span><span style="font-family: NTSPKR+CMEX10; font-size:40px">(cid:88)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:199px; top:37221px; width:61px; height:18px;"><span style="font-family: TKAABP+CMMI10; font-size:10px">max</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(0</span><span style="font-family: TKAABP+CMMI10; font-size:10px">,</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">||</span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">I</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">T
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:255px; top:37221px; width:45px; height:21px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">n </span><span style="font-family: MTIPLH+CMR10; font-size:10px">) </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">I</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">T
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:295px; top:37221px; width:22px; height:21px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">m</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">||</span><span style="font-family: KEWQQA+CMR8; font-size:7px">2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:314px; top:37221px; width:43px; height:21px;"><span style="font-family: KEWQQA+CMR8; font-size:7px">2 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; ||</span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">I</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">T
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:352px; top:37221px; width:58px; height:21px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">n </span><span style="font-family: MTIPLH+CMR10; font-size:10px">) </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8722; </span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">I </span><span style="font-family: QMJINZ+CMMI8; font-size:7px">rand
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:392px; top:37234px; width:5px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">n
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:411px; top:37221px; width:14px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">)</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">||</span><span style="font-family: KEWQQA+CMR8; font-size:7px">2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:421px; top:37229px; width:29px; height:13px;"><span style="font-family: KEWQQA+CMR8; font-size:7px">2 </span><span style="font-family: MTIPLH+CMR10; font-size:10px">+ </span><span style="font-family: TKAABP+CMMI10; font-size:10px">¥á</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:178px; top:37239px; width:19px; height:13px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">m</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">(cid:54)</span><span style="font-family: KEWQQA+CMR8; font-size:7px">=</span><span style="font-family: QMJINZ+CMMI8; font-size:7px">n
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:166px; top:37244px; width:5px; height:7px;"><span style="font-family: QMJINZ+CMMI8; font-size:7px">n
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37272px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">They used translated (T), and translated and cluttered (TC) MNIST images (28x28) on
<br>a 84x84 black background. As features extractor (</span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: MTIPLH+CMR10; font-size:10px">(</span><span style="font-family: TKAABP+CMMI10; font-size:10px">.</span><span style="font-family: MTIPLH+CMR10; font-size:10px">)), they used a pretrain network
<br>on MNIST. As localizer, they used a 100k parameters CNN. Also, they used a spatial
<br>transformer module parameterized for attention (scale, translation, no rotation).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37333px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">They measured a digit to be correctly localized if the overlap (are of intersection divided
<br>by area of union) between the predicted bounding box and groundthruth bounding box is
<br>greater than 0.5. On T they got 100% accuracy. On CT between 75-93%.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37601px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Figure 3.7: Illustration of the dynamics for co-localization. Here are the localization pre-
<br>dicted by the spatial transformer for three of the 100 dataset images after the SGD step
<br>labelled below. By SGD step 180 the model has process has correctly localized the three
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">digits.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37684px; width:115px; height:11px;"><span style="font-family: LMZDQO+CMBX12; font-size:11px">3.3.3 Experiments
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:37712px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Almost all our experiments are made with a batch size of 256, Adam as optimizer, a
<br>learning rate of 3</span><span style="font-family: TKAABP+CMMI10; font-size:10px">e</span><span style="font-family: DNXMRL+CMSY8; font-size:13px">&#8722;</span><span style="font-family: KEWQQA+CMR8; font-size:7px">4</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, a learning rate decay of 0 and a weight decay of 0.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:37768px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">40
<br></span></div><div style="position:absolute; border: figure 1px solid; writing-mode:False; left:91px; top:37383px; width:428px; height:207px;"></div><span style="position:absolute; border: gray 1px solid; left:0px; top:37940px; width:612px; height:792px;"></span>
<div style="position:absolute; top:37940px;"><a name="46">Page 46</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38037px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">On table 3.2, we compare several approaches to a toy problem. The goal for a STN is to
<br>localize where the white pixels on the initial images are, then to generate a zoomed-in view
<br>of the digit.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38099px; width:428px; height:51px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">ConvNet abilities to learn strong spatial invariance.
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">(a) uses a LeNet-5 as classi-
<br>&#64257;er. It is an optimized architecture for the classi&#64257;cation of 28x28 digits on a 32 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">32 back-
<br>ground. In this case, LeNet-5 takes as input downsampled images. This process reduces
<br>the readability of digits. Thus, LeNet-5 is only able to achieve a 85.07% accuracy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38160px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(b) uses a bigger convolutional network (ConvNet100). We want to measure the cost of
<br>the downsampling process. Thus, this network takes as input images of size 100 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">100.
<br>Finally, it is able to achieve a 96.12%, 11% more than LeNet-5 (a). However, we do not
<br>use any data augmentation procedure.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38237px; width:428px; height:38px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">STN abilities
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">(c,d,e,g) uses a LeNet-5 in the same way and apply a transform to the
<br>initial image (100</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿</span><span style="font-family: MTIPLH+CMR10; font-size:10px">100). However, the latter is also downsampled from 100</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿</span><span style="font-family: MTIPLH+CMR10; font-size:10px">100 to 32</span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿</span><span style="font-family: MTIPLH+CMR10; font-size:10px">32
<br>to &#64257;t the input size of the localizer.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38284px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Firstly, we show that even when using bad resolution images, the localizer is able to pro-
<br>duce good spatial transformations, leading to almost the same accuracy as using the full
<br>resolution (d, f, i). Secondly, we show that generating 3, 4 or 6 parameters lead to almost
<br>the same accuracy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38360px; width:428px; height:92px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">(f) uses a WSL with Max Pooling archi-
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:10px">Multi Instance Learning (MIL) abilities
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">tecture as described in subsection 3.1.2. LeNet-5 is transformed to a fully convolutional
<br>network in order to take as input images of size 100 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">100. The &#64257;nal features map is
<br>spatially aggregated by a Max Pooling of the size of the features map (e.g. the output
<br>is of size 1x1x10). This method is the fastest to converge and lead to one of the highest
<br>accuracy (99.15%). We have the same results &#64257;ne tuning a pretrain LeNet-5 on the original
<br>MNIST (e.g. a background of size 32 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">32) with an accuracy of 99.05%.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:38610px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">41
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:38782px; width:612px; height:792px;"></span>
<div style="position:absolute; top:38782px;"><a name="47">Page 47</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:217px; top:38877px; width:27px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Model
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:205px; top:38892px; width:50px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(a) LeNet-5
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:196px; top:38904px; width:70px; height:21px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(b) ConvNet100
<br>(c) STN a&#64259;ne
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:187px; top:38927px; width:87px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(d) STN translation
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:174px; top:38939px; width:114px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(e) STN translation+scale
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:186px; top:38951px; width:89px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(f) MIL MaxPooling
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:152px; top:38963px; width:157px; height:9px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">(g) STN translation+scale+rotation
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:322px; top:38877px; width:137px; height:96px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">Input size Test accuracy top1
<br>32 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">32
<br>100 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">100
<br>32 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">32
<br>32 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">32
<br>32 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">32
<br>100 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">100
<br>32 </span><span style="font-family: SMMPHI+CMSY10; font-size:17px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:9px">32
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:406px; top:38892px; width:22px; height:81px;"><span style="font-family: MTIPLH+CMR10; font-size:9px">85.07
<br>96.12
<br>99.06
<br>99.10
<br>99.10
<br>99.15
<br>99.18
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:38986px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table 3.2: Comparison of di&#64256;erent methods applied on our MNIST Translated dataset
<br>with a background of size 100 </span><span style="font-family: SMMPHI+CMSY10; font-size:18px">¡¿ </span><span style="font-family: MTIPLH+CMR10; font-size:10px">100. We estimate that our results may vary plus or minus
<br>0.10.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:39056px; width:113px; height:14px;"><span style="font-family: LMZDQO+CMBX12; font-size:14px">3.4 Conclusion
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:39089px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this chapter, we studied Weakly Supervised Learning (WSL) approaches which can be
<br>synthesized as follow:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39115px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Multi Instance Learning (MIL) with Max Pooling which considers an image as a bag
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:39137px; width:210px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">of regions and seeks the max scoring region.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39149px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">MIL with k Max Min Pooling (e.g. Weldon) which extends the selection of a single
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:39171px; width:282px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">region to multiple high score regions and low score regions.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39183px; width:412px; height:59px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Spatial Transformer Network (STN) which uses a network (e.g. localizer) that takes
<br>as input the original image and generates a transformed image. Thus, the second
<br>network (e.g. classi&#64257;er) takes as input a invariant representation of the object to
<br>classify.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:39252px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In a &#64257;rst section, we studied the Weldon approach on a small and complex dataset (e.g.
<br>MIT67).
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39278px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">We explained that it is well suited for this kind of datasets where regions of interest
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:39299px; width:276px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">are multiple and negative evidences of classes are present.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39312px; width:382px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Especially, we showed that &#64257;ne tuning is well suited for Weldon architectures.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:39340px; width:242px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In a second section, we studied the STN approach.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:39352px; width:412px; height:66px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Firstly, we explained in which cases STN are used and in which forms.
<br></span><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">Secondly, we compared on a toy dataset (e.g. Translated MNIST) classical Convo-
<br>lutional Neural Networks (CNNs), STN and MIL with Max Pooling approaches in
<br>term of spatial invariance capacity. Thus, we showed that STN was able to generate
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:39452px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">42
<br></span></div><span style="position:absolute; border: black 1px solid; left:146px; top:38877px; width:318px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38877px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38877px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38877px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38877px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38889px; width:318px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38891px; width:318px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38891px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38891px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38891px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38891px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38903px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38903px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38903px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38903px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38915px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38915px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38915px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38915px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38927px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38927px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38927px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38927px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38939px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38939px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38939px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38939px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38951px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38951px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38951px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38951px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38963px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:316px; top:38963px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:38963px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:465px; top:38963px; width:0px; height:11px;"></span>
<span style="position:absolute; border: black 1px solid; left:146px; top:38975px; width:318px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:39624px; width:612px; height:792px;"></span>
<div style="position:absolute; top:39624px;"><a name="48">Page 48</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:39721px; width:401px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">invariant representations of the digits in order to achieve better accuracy than CNNs
<br>and the same accuracy than MIL.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:40294px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">43
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:40466px; width:612px; height:792px;"></span>
<div style="position:absolute; top:40466px;"><a name="49">Page 49</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:40638px; width:133px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Conclusion
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:40709px; width:278px; height:24px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Summary of Contributions
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">tures for classifying medium and small datasets of images.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:249px; top:40709px; width:271px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">In this master¡¯s thesis, we studied deep learning architec-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:40735px; width:412px; height:18px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In a &#64257;rst chapter, we explained how Convolutional Neural Networks can achieve such
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:119px; top:40757px; width:70px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">good accuracy.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:40769px; width:412px; height:46px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In a second chapter, we showed the e&#64259;ciency of the Fine Tuning approach on this
<br>kind of dataset. We also explained our winning solution to the DSG online challenge
<br>based on a bootstrap of &#64257;ne tuned InceptionV3.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:108px; top:40817px; width:412px; height:59px;"><span style="font-family: SMMPHI+CMSY10; font-size:18px">&#8226; </span><span style="font-family: MTIPLH+CMR10; font-size:10px">In a last chapter, we showed the advantages and drawbacks of Weakly Supervised
<br>Learning approaches such as Multi Instance Learning (MIL) and Spatial Transformer
<br>Networks (STN). Using Fine Tuning, we also improved Weldon, a certain kind of MIL
<br>model.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:40901px; width:428px; height:78px;"><span style="font-family: WCLHMO+CMBX10; font-size:10px">Future Directions
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">In future studies, we would like to adapt the methods developed
<br>during this study to multi-modal datasets made of images and texts. Furthermore, we
<br>would like to apply Fine Tuning and Weakly Supervised Learning on the last architec-
<br>tures such as Wide Residual Networks. Finally, we would like to explore hybrid weakly
<br>supervised architectures counteracting the drawbacks of MIL and STN, and seeking im-
<br>provements.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:41136px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">44
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:41308px; width:612px; height:792px;"></span>
<div style="position:absolute; top:41308px;"><a name="50">Page 50</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:235px; top:41597px; width:140px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Appendices
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:41978px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">45
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:42150px; width:612px; height:792px;"></span>
<div style="position:absolute; top:42150px;"><a name="51">Page 51</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:42320px; width:124px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Appendix A
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:42374px; width:104px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Overfeat
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:123px; top:42447px; width:41px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Layer id
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:42447px; width:54px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Layer type
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:390px; top:42447px; width:98px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Parameters number
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:126px; top:42463px; width:33px; height:212px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">(0):
<br>(1):
<br>(3):
<br>(4):
<br>(6):
<br>(7):
<br>(9):
<br>(11):
<br>(13):
<br>(15):
<br>(16):
<br>(18):
<br>(20):
<br>(21):
<br>Total :
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:228px; top:42463px; width:97px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Image (3, 221, 221)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:191px; top:42478px; width:171px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Convolution (96, 3x7x7, 2x2, 0x0)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:423px; top:42478px; width:32px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">14,208
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:221px; top:42492px; width:111px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">MaxPooling (3x3,3x3)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:198px; top:42506px; width:158px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Convolution (256, 96x3x3, 7x7)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:416px; top:42506px; width:46px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">1,204,480
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:221px; top:42521px; width:111px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:177px; top:42535px; width:200px; height:54px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">Convolution (512, 256x3x3, 1x1, 1x1)
<br>Convolution (512, 512x3x3, 1x1, 1x1)
<br>Convolution (1024, 512x3x3, 1x1, 1x1)
<br>Convolution (1024, 1024x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:221px; top:42592px; width:111px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">MaxPooling (3x3,3x3)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:195px; top:42598px; width:162px; height:48px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">FullyConnected (25600 </span><span style="font-family: SMMPHI+CMSY10; font-size:20px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:11px">4096)
<br>FullyConnected (4096 </span><span style="font-family: SMMPHI+CMSY10; font-size:20px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:11px">4096)
<br>FullyConnected (4096 </span><span style="font-family: SMMPHI+CMSY10; font-size:20px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:11px">1000)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:255px; top:42650px; width:44px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">SoftMax
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:11px">9 layers
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:416px; top:42535px; width:46px; height:54px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">1,180,160
<br>2,359,808
<br>4,719,616
<br>9,438,208
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:410px; top:42607px; width:58px; height:40px;"><span style="font-family: MTIPLH+CMR10; font-size:11px">104,861,696
<br>16,781,312
<br>4,097,000
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:405px; top:42664px; width:67px; height:11px;"><span style="font-family: WCLHMO+CMBX10; font-size:11px">144,656,488
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:42689px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table A.1: Deep architecture used in our experiments. A ReLU non-linearity follows each
<br>convolutional and fully-connected layers, beside the last one. Convolution (512, 512x3x3,
<br>1x1, 1x1) means 512 &#64257;lters (e.g. 512 output channels), a kernel size of 256x3x3, 1 step of
<br>the convolution to the width and height dimensions, 1 additional zero padded per width to
<br>the input, 1 per hight. MaxPooling (2,2,2,2) means a 2D pooling operation on 2x2 pixels
<br>neighborhood, by step size of 2x2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:42820px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">46
<br></span></div><span style="position:absolute; border: black 1px solid; left:116px; top:42446px; width:378px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42446px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42446px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42446px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42446px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42460px; width:378px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42462px; width:378px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42463px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42463px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42463px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42463px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42477px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42477px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42477px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42477px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42491px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42491px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42491px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42491px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42506px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42506px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42506px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42506px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42520px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42520px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42520px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42520px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42534px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42534px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42534px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42534px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42549px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42549px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42549px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42549px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42563px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42563px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42563px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42563px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42577px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42577px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42577px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42577px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42592px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42592px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42592px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42592px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42606px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42606px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42606px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42606px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42620px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42620px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42620px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42620px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42635px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42635px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42635px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42635px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42649px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42649px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42649px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42649px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42663px; width:378px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42664px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:170px; top:42664px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:383px; top:42664px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:495px; top:42664px; width:0px; height:14px;"></span>
<span style="position:absolute; border: black 1px solid; left:116px; top:42678px; width:378px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:42992px; width:612px; height:792px;"></span>
<div style="position:absolute; top:42992px;"><a name="52">Page 52</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:43162px; width:123px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Appendix B
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:43216px; width:76px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Vgg16
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:189px; top:43293px; width:27px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Layer id
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:269px; top:43293px; width:35px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Layer type
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:357px; top:43293px; width:65px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Parameters number
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:192px; top:43304px; width:22px; height:243px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">(0):
<br>(1):
<br>(3):
<br>(5):
<br>(6):
<br>(8):
<br>(10):
<br>(11):
<br>(13):
<br>(15):
<br>(17):
<br>(18):
<br>(20):
<br>(22):
<br>(24):
<br>(25):
<br>(27):
<br>(29):
<br>(31):
<br>(32):
<br>(34):
<br>(35):
<br>(37):
<br></span><span style="font-family: MTIPLH+CMR10; font-size:7px">(38):
<br>(40):
<br>Total :
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:255px; top:43304px; width:64px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Image (3, 224, 224)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:228px; top:43313px; width:116px; height:16px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Convolution (64, 3x3x3, 1x1, 1x1)
<br>Convolution (64, 64x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:43332px; width:73px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:225px; top:43342px; width:123px; height:16px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Convolution (128, 64x3x3, 1x1, 1x1)
<br>Convolution (128, 128x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:43360px; width:73px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:225px; top:43370px; width:123px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Convolution (256, 128x3x3, 1x1, 1x1)
<br>Convolution (256, 256x3x3, 1x1, 1x1)
<br>Convolution (256, 256x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:43398px; width:73px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:225px; top:43408px; width:123px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Convolution (512, 256x3x3, 1x1, 1x1)
<br>Convolution (512, 512x3x3, 1x1, 1x1)
<br>Convolution (512, 512x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:43436px; width:73px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:225px; top:43445px; width:123px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Convolution (512, 512x3x3, 1x1, 1x1)
<br>Convolution (512, 512x3x3, 1x1, 1x1)
<br>Convolution (512, 512x3x3, 1x1, 1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:43473px; width:73px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">MaxPooling (2x2,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:262px; top:43492px; width:50px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Dropout (50%)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:233px; top:43477px; width:106px; height:50px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">FullyConnected (25088 </span><span style="font-family: SMMPHI+CMSY10; font-size:13px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:7px">4096)
<br>FullyConnected (4096 </span><span style="font-family: SMMPHI+CMSY10; font-size:13px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:7px">4096)
<br></span><span style="font-family: MTIPLH+CMR10; font-size:7px">FullyConnected (4096 </span><span style="font-family: SMMPHI+CMSY10; font-size:13px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:7px">1000)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:262px; top:43511px; width:50px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">Dropout (50%)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:270px; top:43530px; width:33px; height:17px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">SoftMax
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:7px">16 layers
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:379px; top:43313px; width:21px; height:16px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">1,792
<br>36,928
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:377px; top:43342px; width:24px; height:16px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">73,856
<br>147,584
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:377px; top:43370px; width:24px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">295,168
<br>590,080
<br>590,080
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:374px; top:43408px; width:30px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">1,180,160
<br>2,359,808
<br>2,359,808
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:374px; top:43445px; width:30px; height:26px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">2,359,808
<br>2,359,808
<br>2,359,808
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:370px; top:43483px; width:38px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">102,764,544
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:372px; top:43502px; width:34px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">16,781,312
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:374px; top:43520px; width:30px; height:7px;"><span style="font-family: MTIPLH+CMR10; font-size:7px">4,097,000
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:367px; top:43539px; width:44px; height:7px;"><span style="font-family: WCLHMO+CMBX10; font-size:7px">138,357,544
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:43559px; width:428px; height:78px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table B.1: Deep architecture used in our experiments. A ReLU non-linearity follows each
<br>convolutional and fully-connected layers, beside the last one. Convolution (512, 512x3x3,
<br>1x1, 1x1) means 512 &#64257;lters (e.g. 512 output channels), a kernel size of 256x3x3, 1 step of
<br>the convolution to the width and height dimensions, 1 additional zero padded per width to
<br>the input, 1 per hight. MaxPooling (2,2,2,2) means a 2D pooling operation on 2x2 pixels
<br>neighborhood, by step size of 2x2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:43662px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">47
<br></span></div><span style="position:absolute; border: black 1px solid; left:185px; top:43292px; width:241px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43292px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43292px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43292px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43292px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43302px; width:241px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43303px; width:241px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43304px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43304px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43304px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43304px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43313px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43313px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43313px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43313px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43322px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43322px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43322px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43322px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43332px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43332px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43332px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43332px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43341px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43341px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43341px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43341px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43351px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43351px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43351px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43351px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43360px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43360px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43360px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43360px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43369px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43369px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43369px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43369px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43379px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43379px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43379px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43379px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43388px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43388px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43388px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43388px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43398px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43398px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43398px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43398px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43407px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43407px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43407px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43407px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43416px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43416px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43416px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43416px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43426px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43426px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43426px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43426px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43435px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43435px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43435px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43435px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43445px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43445px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43445px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43445px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43454px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43454px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43454px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43454px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43463px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43463px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43463px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43463px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43473px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43473px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43473px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43473px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43482px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43482px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43482px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43482px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43492px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43492px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43492px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43492px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43501px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43501px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43501px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43501px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43511px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43511px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43511px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43511px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43520px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43520px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43520px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43520px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43529px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43529px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43529px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43529px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43539px; width:241px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43539px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:221px; top:43539px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:353px; top:43539px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:426px; top:43539px; width:0px; height:9px;"></span>
<span style="position:absolute; border: black 1px solid; left:185px; top:43549px; width:241px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:43834px; width:612px; height:792px;"></span>
<div style="position:absolute; top:43834px;"><a name="53">Page 53</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:44004px; width:123px; height:20px;"><span style="font-family: LMZDQO+CMBX12; font-size:20px">Appendix C
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:44058px; width:149px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">InceptionV3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:166px; top:44135px; width:29px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Layer id
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:168px; top:44147px; width:24px; height:214px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">(0):
<br>(1):
<br>(2):
<br>(4):
<br>(5):
<br>(7):
<br>(8):
<br>(10):
<br>(11):
<br>(12):
<br>(14):
<br>(15):
<br>(17):
<br>(18):
<br>(19):
<br>(20):
<br>(20):
<br>(21):
<br>(22):
<br>(23):
<br>Total :
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:265px; top:44135px; width:38px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Layer type
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:250px; top:44147px; width:70px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Image (3, 299, 299)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:374px; top:44135px; width:71px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Parameters number
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:209px; top:44157px; width:152px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Convolution (32, 3x3x3, 2x2, 0x0) no bias
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:247px; top:44168px; width:75px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Batch Normalization
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:207px; top:44178px; width:156px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Convolution (32, 32x3x3, 1x1, 0x0) no bias
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:247px; top:44188px; width:75px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Batch Normalization
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:207px; top:44199px; width:156px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Convolution (64, 32x3x3, 1x1, 1x1) no bias
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:245px; top:44209px; width:80px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Batch Normalization
<br>MaxPooling (3x3,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:207px; top:44230px; width:156px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Convolution (80, 64x3x3, 1x1, 0x0) no bias
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:247px; top:44240px; width:75px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Batch Normalization
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:205px; top:44250px; width:160px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Convolution (192, 80x3x3, 1x1, 0x0) no bias
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:245px; top:44261px; width:80px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Batch Normalization
<br>MaxPooling (3x3,2x2)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:259px; top:44281px; width:52px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">4 x Inception1
<br>4 x Inception2
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:266px; top:44302px; width:38px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">Inception3
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:259px; top:44312px; width:52px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">2 x Inception4
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:245px; top:44323px; width:80px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">MaxPooling (8x8,1x1)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:228px; top:44327px; width:112px; height:14px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">FullyConnected (2048 </span><span style="font-family: SMMPHI+CMSY10; font-size:14px">¡æ </span><span style="font-family: MTIPLH+CMR10; font-size:8px">1000)
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:266px; top:44343px; width:36px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">SoftMax
<br></span><span style="font-family: WCLHMO+CMBX10; font-size:8px">42 layers
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:400px; top:44157px; width:18px; height:39px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">864
<br>64*
<br>9,216
<br>64*
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:398px; top:44199px; width:23px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">18,432
<br>128*
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:400px; top:44230px; width:18px; height:18px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">5,120
<br>160*
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:396px; top:44250px; width:27px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">138,240
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:401px; top:44261px; width:16px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">384*
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:408px; top:44281px; width:2px; height:39px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">-
<br>-
<br>-
<br>-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:393px; top:44333px; width:33px; height:8px;"><span style="font-family: MTIPLH+CMR10; font-size:8px">2,065,392
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:388px; top:44354px; width:43px; height:8px;"><span style="font-family: WCLHMO+CMBX10; font-size:8px">23,816,528
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:44375px; width:428px; height:105px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Table C.1: Inception architecture used in our experiments. A ReLU non-linearity follows
<br>each convolutional and fully-connected layers, beside the last one. * means that the param-
<br>eters are not learned by backpropagation. For layers Inception1, Inception2, Inception3
<br>and Inception4, please refer to the original paper [44]. Convolution (512, 512x3x3, 1x1,
<br>1x1) means 512 &#64257;lters (e.g. 512 output channels), a kernel size of 256x3x3, 1 step of the
<br>convolution to the width and height dimensions, 1 additional zero padded per width to
<br>the input, 1 per hight. MaxPooling (2,2,2,2) means a 2D pooling operation on 2x2 pixels
<br>neighborhood, by step size of 2x2.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:44504px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">48
<br></span></div><span style="position:absolute; border: black 1px solid; left:161px; top:44134px; width:288px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44135px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44135px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44135px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44135px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44145px; width:288px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44146px; width:288px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44147px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44147px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44147px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44147px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44157px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44157px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44157px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44157px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44167px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44167px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44167px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44167px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44178px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44178px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44178px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44178px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44188px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44188px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44188px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44188px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44198px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44198px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44198px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44198px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44209px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44209px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44209px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44209px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44219px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44219px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44219px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44219px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44229px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44229px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44229px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44229px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44240px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44240px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44240px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44240px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44250px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44250px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44250px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44250px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44260px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44260px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44260px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44260px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44270px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44270px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44270px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44270px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44281px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44281px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44281px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44281px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44291px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44291px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44291px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44291px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44301px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44301px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44301px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44301px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44312px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44312px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44312px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44312px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44322px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44322px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44322px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44322px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44332px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44332px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44332px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44332px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44343px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44343px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44343px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44343px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44353px; width:288px; height:0px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44353px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:200px; top:44353px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:370px; top:44353px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:450px; top:44353px; width:0px; height:10px;"></span>
<span style="position:absolute; border: black 1px solid; left:161px; top:44364px; width:288px; height:0px;"></span>
<span style="position:absolute; border: gray 1px solid; left:0px; top:44676px; width:612px; height:792px;"></span>
<div style="position:absolute; top:44676px;"><a name="54">Page 54</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:44848px; width:156px; height:24px;"><span style="font-family: LMZDQO+CMBX12; font-size:24px">Bibliography
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:44919px; width:422px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[1] Sandra Avila, Nicolas Thome, Matthieu Cord, Eduardo Valle, and Arnaldo De A
<br>Ara¢¥uJo. Pooling in image representation: The visual codeword point of view. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Com-
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">puter Vision and Image Understanding</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 117(5):453&#8211;465, 2013.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:44967px; width:422px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[2] Yoshua Bengio, Pascal Lamblin, Dan Popovici, Hugo Larochelle, et al. Greedy layer-
<br>wise training of deep networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Advances in neural information processing systems</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>19:153, 2007.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45014px; width:422px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[3] Hakan Bilen and Andrea Vedaldi. Weakly supervised deep detection networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">CVPR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45028px; width:104px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">abs/1511.02853, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45048px; width:422px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[4] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101&#8211;mining discrimi-
<br>native components with random forests. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">European Conference on Computer Vision</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>pages 446&#8211;461. Springer, 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45095px; width:38px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[5] Jean
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:153px; top:45095px; width:37px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">DaRolt.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:234px; top:45095px; width:21px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">How
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:272px; top:45095px; width:7px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">is
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:255px; top:45095px; width:47px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">a
<br>features?
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:145px; top:45109px; width:23px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">learn
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:191px; top:45109px; width:42px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">invariant
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45109px; width:411px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">to
<br></span><span style="font-family: LDTEBE+CMTT10; font-size:10px">How-is-a-convolutional-neural-network-able-to-learn-invariant-features/
<br>answer/Jean-Da-Rolt?srid=NRAy</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, year = 2016, note =.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:319px; top:45095px; width:63px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">convolutional
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:394px; top:45095px; width:126px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">able
<br>neural
<br></span><span style="font-family: LDTEBE+CMTT10; font-size:10px">https://www.quora.com/
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:446px; top:45095px; width:37px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45156px; width:422px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[6] Sander Dieleman, Je&#64256;rey De Fauw, and Koray Kavukcuoglu. Exploiting cyclic sym-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45170px; width:190px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">metry in convolutional neural networks.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45190px; width:422px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[7] Thibaut Durand, Nicolas Thome, and Matthieu Cord. Weldon: Weakly supervised
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">learning of deep convolutional neural networks. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings of the IEEE Interna-
<br>tional Conference on Computer Vision and Pattern Recognition</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45238px; width:422px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[8] Abel Gonzalez-Garcia, Davide Modolo, and Vittorio Ferrari. Do semantic parts emerge
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45251px; width:353px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">in convolutional neural networks? </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">arXiv preprint arXiv:1607.03738</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:97px; top:45272px; width:422px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[9] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Deep Learning</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. MIT Press,
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45285px; width:199px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2016. Book in preparation for MIT Press.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:45346px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">49
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:45518px; width:612px; height:792px;"></span>
<div style="position:absolute; top:45518px;"><a name="55">Page 55</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45615px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[10] Jiuxiang Gu, Zhenhua Wang, Jason Kuen, Lianyang Ma, Amir Shahroudy, Bing Shuai,
<br>Ting Liu, Xingxing Wang, and Gang Wang. Recent advances in convolutional neural
<br>networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">CoRR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, abs/1512.07108, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45662px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[11] Mrinal Haloi. Tra&#64259;c sign classi&#64257;cation using deep inception based convolutional net-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45676px; width:222px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">works. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">arXiv preprint arXiv:1511.02992</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45696px; width:302px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[12] Donald Hebb. 0.(1949) the organization of behavior, 1968.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45716px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[13] Geo&#64256;rey E Hinton, Simon Osindero, and Yee-Whye Teh. A fast learning algorithm
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45730px; width:307px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">for deep belief nets. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Neural computation</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 18(7):1527&#8211;1554, 2006.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45750px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[14] Sergey Io&#64256;e and Christian Szegedy. Batch normalization: Accelerating deep network
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45764px; width:357px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">training by reducing internal covariate shift. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">JMLR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, abs/1502.03167, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45784px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer net-
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">works. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Advances in Neural Information Processing Systems</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 2008&#8211;2016, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45818px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[16] Angjoo Kanazawa, Abhishek Sharma, and David Jacobs. Locally scale-invariant con-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45831px; width:242px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">volutional neural networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">NIPS Workshop</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45852px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[17] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45865px; width:132px; height:10px;"><span style="font-family: SUPZPA+CMTI10; font-size:10px">ICLR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, abs/1412.6980, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45886px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[18] Alex Krizhevsky, Ilya Sutskever, and Geo&#64256;rey E. Hinton. Imagenet classi&#64257;cation with
<br>deep convolutional neural networks. In F. Pereira, C. J. C. Burges, L. Bottou, and
<br>K. Q. Weinberger, editors, </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Advances in Neural Information Processing Systems 25</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>pages 1097&#8211;1105. Curran Associates, Inc., 2012.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45947px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[19] Alex Krizhevsky, Ilya Sutskever, and Geo&#64256;rey E Hinton. Imagenet classi&#64257;cation with
<br>In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Advances in neural information processing
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:45960px; width:175px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">deep convolutional neural networks.
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">systems</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 1097&#8211;1105, 2012.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:45994px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[20] Yann Le Cun, Ido Kanter, and Sara A Solla. Eigenvalues of covariance matrices:
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46008px; width:399px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Application to neural-network learning. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Physical Review Letters</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 66(18):2396, 1991.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46028px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[21] Yann LeCun and Yoshua Bengio. Convolutional networks for images, speech, and time
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46042px; width:376px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">series. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">The handbook of brain theory and neural networks</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 3361(10):1995, 1995.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46062px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[22] Yann LeCun, L¢¥eon Bottou, Yoshua Bengio, and Patrick Ha&#64256;ner. Gradient-based
<br>learning applied to document recognition. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings of the IEEE</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 86(11):2278&#8211;2324,
<br>1998.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46109px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[23] Zachary C Lipton, David C Kale, Charles Elkan, Randall Wetzell, Sharad Vikram, Ju-
<br>lian McAuley, Randall C Wetzell, Zhanglong Ji, Balakrishnan Narayaswamy, Cheng-I
<br>Wang, et al. The mythos of model interpretability. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">IEEE Spectrum</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:46188px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">50
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:46360px; width:612px; height:792px;"></span>
<div style="position:absolute; top:46360px;"><a name="56">Page 56</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46457px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[24] Chang Liu, Yu Cao, Yan Luo, Guanling Chen, Vinod Vokkarane, and Yunsheng Ma.
<br>Deepfood: Deep learning-based food image recognition for computer-aided dietary
<br>assessment.
<br>In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">International Conference on Smart Homes and Health Telematics</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>pages 37&#8211;48. Springer, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46518px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[25] David G Lowe. Object recognition from local scale-invariant features. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Computer
<br>vision, 1999. The proceedings of the seventh IEEE international conference on</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, vol-
<br>ume 2, pages 1150&#8211;1157. Ieee, 1999.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46565px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[26] Laurens van der Maaten and Geo&#64256;rey Hinton. Visualizing data using t-sne. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Journal
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46579px; width:267px; height:10px;"><span style="font-family: SUPZPA+CMTI10; font-size:10px">of Machine Learning Research</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 9(Nov):2579&#8211;2605, 2008.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46599px; width:428px; height:65px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[27] Austin Meyers, Nick Johnston, Vivek Rathod, Anoop Korattikara, Alex Gorban,
<br>Nathan Silberman, Sergio Guadarrama, George Papandreou, Jonathan Huang, and
<br>Kevin P Murphy. Im2calories: towards an automated mobile vision food diary. In
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings of the IEEE International Conference on Computer Vision</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 1233&#8211;
<br>1241, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46673px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[28] Dmytro Mishkin, Nikolay Sergievskiy, and Jiri Matas. Systematic evaluation of cnn
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46687px; width:318px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">advances on the imagenet. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">arXiv preprint arXiv:1606.02228</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46707px; width:317px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[29] Alexander Mordvintsev, Christopher Olah, and Mike Tyka.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:419px; top:46707px; width:100px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Inceptionism: Going
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46721px; width:368px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">deeper into neural networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Google Research Blog. Retrieved June</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 20, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46741px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[30] Michael Nielsen. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Neural Networks and Deep Learning</span><span style="font-family: MTIPLH+CMR10; font-size:10px">. Determination Press, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46755px; width:150px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[Online; accessed 30-July-2016].
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46775px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[31] Maxime Oquab, L¢¥eon Bottou, Ivan Laptev, and Josef Sivic. Is object localization for
<br>free?-weakly-supervised learning with convolutional neural networks. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings
<br>of the IEEE Conference on Computer Vision and Pattern Recognition</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 685&#8211;694,
<br>2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46836px; width:428px; height:11px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[32] Aaditya Prakash. One by one [1x1] Convolution - Counter-intuitively useful. </span><span style="font-family: LDTEBE+CMTT10; font-size:10px">http://
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46850px; width:400px; height:11px;"><span style="font-family: LDTEBE+CMTT10; font-size:10px">iamaaditya.github.io/2016/03/one-by-one-convolution</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, year = 2016, note =.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46870px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[33] Ariadna Quattoni and Antonio Torralba. Recognizing indoor scenes. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Computer
<br>Vision and Pattern Recognition, 2009. CVPR 2009. IEEE Conference on</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 413&#8211;
<br></span><span style="font-family: MTIPLH+CMR10; font-size:10px">420. IEEE, 2009.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46917px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[34] Marc Aurelio Ranzato, Fu Jie Huang, Y-Lan Boureau, and Yann LeCun. Unsupervised
<br>In
<br>learning of invariant feature hierarchies with applications to object recognition.
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Computer Vision and Pattern Recognition, 2007. CVPR¡¯07. IEEE Conference on</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>pages 1&#8211;8. IEEE, 2007.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:46978px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[35] F. Rosenblatt. The perceptron: A probabilistic model for information storage and
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:46992px; width:329px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">organization in the brain. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Psychological Review</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 65(6):386?408, 1958.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:47030px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">51
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:47202px; width:612px; height:792px;"></span>
<div style="position:absolute; top:47202px;"><a name="57">Page 57</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47299px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[36] David E Rumelhart, Geo&#64256;rey E Hinton, and Ronald J Williams. Learning represen-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47312px; width:329px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">tations by back-propagating errors. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Cognitive modeling</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 5(3):1, 1988.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47333px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[37] Juergen Schmidhuber. Deep learning in neural networks: An overview. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">arXiv</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, Apr
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47346px; width:24px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47366px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[38] Pierre Sermanet, David Eigen, Xiang Zhang, Micha¡§el Mathieu, Rob Fergus, and Yann
<br>LeCun. Overfeat: Integrated recognition, localization and detection using convolu-
<br>tional networks. 2013.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47414px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[39] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Deep inside convolutional
<br>networks: Visualizing image classi&#64257;cation models and saliency maps. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">ICLR Workshop</span><span style="font-family: MTIPLH+CMR10; font-size:10px">,
<br>2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47461px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[40] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47475px; width:143px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">scale image recognition. 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47495px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[41] Nitish Srivastava, Geo&#64256;rey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan
<br>Salakhutdinov. Dropout: a simple way to prevent neural networks from over&#64257;tting.
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Journal of Machine Learning Research</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 15(1):1929&#8211;1958, 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47543px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[42] Ilya Sutskever, James Martens, George E Dahl, and Geo&#64256;rey E Hinton. On the
<br></span><span style="font-family: SUPZPA+CMTI10; font-size:10px">ICML (3)</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 28:1139&#8211;
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47556px; width:299px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">importance of initialization and momentum in deep learning.
<br>1147, 2013.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47590px; width:428px; height:51px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[43] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir
<br>Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
<br>deeper with convolutions. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings of the IEEE Conference on Computer Vision
<br>and Pattern Recognition</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 1&#8211;9, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47651px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[44] Christian Szegedy, Vincent Vanhoucke, Sergey Io&#64256;e, Jonathon Shlens, and Zbigniew
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47665px; width:385px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">Wojna. Rethinking the inception architecture for computer vision. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">CVPR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47685px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[45] Yaniv Taigman, Ming Yang, Marc¡¯Aurelio Ranzato, and Lior Wolf. Deepface: Closing
<br>the gap to human-level performance in face veri&#64257;cation. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Proceedings of the IEEE
<br>Conference on Computer Vision and Pattern Recognition</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 1701&#8211;1708, 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47732px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[46] Andreas Veit, Michael J. Wilber, and Serge J. Belongie. Residual networks are expo-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:47746px; width:376px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">nential ensembles of relatively shallow networks. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">CoRR</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, abs/1605.06431, 2016.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47766px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[47] Xin Wang, Devinder Kumar, Nicolas Thome, Matthieu Cord, and Frederic Precioso.
<br>Recipe recognition with large multimodal food dataset. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Multimedia &amp; Expo Work-
<br>shops (ICMEW), 2015 IEEE International Conference on</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 1&#8211;6. IEEE, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:47814px; width:428px; height:24px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[48] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional net-
<br>works. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">European Conference on Computer Vision</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 818&#8211;833. Springer, 2014.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:47872px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">52
<br></span></div><span style="position:absolute; border: gray 1px solid; left:0px; top:48044px; width:612px; height:792px;"></span>
<div style="position:absolute; top:48044px;"><a name="58">Page 58</a></div>
<div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:48141px; width:428px; height:38px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[49] Matthew D Zeiler, Dilip Krishnan, Graham W Taylor, and Rob Fergus. Deconvolu-
<br>tional networks. In </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE
<br>Conference on</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, pages 2528&#8211;2535. IEEE, 2010.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:91px; top:48188px; width:428px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">[50] Sixin Zhang, Anna Choromanska, and Yann LeCun. Deep learning with elastic aver-
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:114px; top:48202px; width:191px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">aging SGD. </span><span style="font-family: SUPZPA+CMTI10; font-size:10px">NIPS</span><span style="font-family: MTIPLH+CMR10; font-size:10px">, abs/1412.6651, 2015.
<br></span></div><div style="position:absolute; border: textbox 1px solid; writing-mode:lr-tb; left:300px; top:48714px; width:10px; height:10px;"><span style="font-family: MTIPLH+CMR10; font-size:10px">53
<br></span></div>